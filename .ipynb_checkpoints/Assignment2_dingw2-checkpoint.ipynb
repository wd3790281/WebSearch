{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "0016b8156bb9f0ae55138f69fae4b4d4b356c292be101c8c852a30b0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP90042 Assignment #2: Cross-language Information Retreival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "ea618958c35a6254f4fd2452237aed3c8aaae546b030e610dfd0c9d0"
   },
   "source": [
    "```Student Name: Ding Wang\n",
    "Student ID: 722822```\n",
    "\n",
    "Please do not edit this field. It must be left as is for use in marking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "050ba382db22b336aff67e6fee5c8fd07dd2ffdee71f39d7d4b25980"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9fdb67b5bc2482368bc0b03a1877225f5316af7dc1010820d703d575"
   },
   "source": [
    "**Due date**: 5pm, Wed 25th May\n",
    "\n",
    "**Submission method**: see LMS\n",
    "\n",
    "**Submission materials**: completed copy of this ipython notebook. *Do not upload the corpora*, you can assume these files will be extracted into the same folder as the code.  *Do not use absolute paths* such as paths to files on your machine, use *relative paths* instead. *Include output* in the ipython notebook, and ensure your code can run sucessfully from scratch (e.g., use *Kernel -> Restart and Run All* menu option to test.)\n",
    "\n",
    "**Late submissions**: -10% per day, no late submissions after the first week\n",
    "\n",
    "**Marks**: 25% of mark for class\n",
    "\n",
    "**Overview**: For this project, you'll be building a cross language information retreival system (CLIR), which is capable of searching for important text documents written in a English given a query in German, and displaying the results in German. This incorporates machine translation, information retrieval using a vector space model, and IR evaluation. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "**Materials**: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages as used for workshops and the previous assignments. The main part of this assignment should be done entirely in python, using built-in python packages and the packages used in the class (NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim). Please do no use any other 3rd party packages. Note that some choices of the *extended part* of the assignment may require you to work with other tools, such as 3rd party translation tools or retreival engines, in which case this part of the assignment will require working in another environment, although you should report your results in the ipython notebook and attach your other files (code, shell scripts etc) with your submission.   \n",
    "\n",
    "You are encouraged to make use of the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. Adapting code from the class notebooks is permitted.\n",
    "\n",
    "**Data**: The other data you will need are:\n",
    "  - parallel German-English corpus.  This data comes from the Europarl corpus, which is a collection of debates held in the EU parliament over many years, translated into several EU languages. This data was collated as part of the annual WMT translation competitions. The data has been tokenised, sentence aligned using the Church-Gale algorithm. \n",
    "  - CLIR evaluation corpus, comprising a large collection of English documents sourced from Wikipedia along with several German query strings and relevance judgements for each document and query pair. \n",
    "  \n",
    "The data files are as follows:\n",
    "  - *bitext-small.{en,de}* is the sentence-aligned parallel corpus, of a small enough size for you to use for developing word-alignment tools. You may wish to work on a smaller subset during code development.\n",
    "  - *bitext-large.{en,de}* is a much larger version of the above corpus, for language modelling and translation.\n",
    "  - *newstest2013.{en,de}* is a separate small parallel corpus reserved for evaluation.\n",
    "  - *dev.{docs,queries,qrel}* is a set of documents in English, queries in German and query relevance judgements, for the development of the IR components and evaluation.\n",
    "  \n",
    "For details of the datasets, please see:\n",
    "> Philipp Koehn. *Europarl: A Parallel Corpus for Statistical Machine Translation* MT Summit 2005.   \n",
    "\n",
    "and\n",
    "> Shigehiko Schamoni, Felix Hieber, Artem Sokolov, Stefan Riezler. *Learning Translational and Knowledge-based Similarities from Relevance Rankings for Cross-Language Retrieval* ACL 2014.\n",
    "\n",
    "You can find the PDFs for both papers online with a quick web search.\n",
    "\n",
    "**Evaluation**: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 15 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. Indexing or translation steps might taken longer than this, however you should upload with your submission preprocessed files (e.g., text files, pickled python structures) to keep the runtime modest.\n",
    "You will be marked not only on the correctness of your methods, but also on your explanation and analysis. \n",
    "\n",
    "Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "**Updates**: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "**Academic Misconduct**: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s [Academic Misconduct policy](http://academichonesty.unimelb.edu.au/policy.html) where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "478d96b5e15fdd6436c7ed60c1a481a0182513254e40ed58afb9de25"
   },
   "source": [
    "### Warning: File encodings and tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "230df77aff47735078cc012ffe8f0c9b306eed242cf12a6a9627b3ce"
   },
   "source": [
    "All the text files are encoded in *utf-8*, which requires some care in using with python strings, the nltk tools and jupyter.  Please use the following method to convert strings into ASCII by escaping the special symbols. Be careful to do the conversion after tokenisation, as the escaped tags might interfere with the NLTK tokenizer and get treated as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "signature": "437c3bdebe6604b2d5585737b38964537a2532b87b22feda023cec5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seit',\n",
       " 'damals',\n",
       " 'ist',\n",
       " 'er',\n",
       " 'auf',\n",
       " '\\\\xfcber',\n",
       " '10.000',\n",
       " 'punkte',\n",
       " 'gestiegen',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]\n",
    "\n",
    "tokenize(\"Seit damals ist er auf über 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e37ad4a972e433b3fd46688eedcb370e36f89697b8650c431a70d154"
   },
   "source": [
    "# Part 1: CLIR engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f8f2b72ece498207ca426ebae4d2e85070cf9df648b3d40b239e61fd"
   },
   "source": [
    "The project has two parts: first you must build a CLIR engine, comprising information retrieval and translation components, and evaluate its accuracy. Next you will propose an extension to a component of the system. In each step you will need to justify your modelling and implementation decisions, and evaluate the quality of the outputs of the system, and at the end you will need to reflect on the overall outcomes.\n",
    "\n",
    "The CLIR system will involve:\n",
    " - translating queries from German into English, the language of our text collection, using word-based translation\n",
    " - once the queries and the documents are in the same language, search over the document collection using BM25\n",
    " - evaluate the quality of ranked retreival results using the query relevance judgements\n",
    "\n",
    "Note that you could try translating the text collection into German instead, but for this project we'll stick with translating the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a49612409146da00d52357b0a3457f3dfcf5481948a2531587571e71"
   },
   "source": [
    "Building the CLIR engine is the majority of work in the assignment, and  constitutes 70% of the assignment mark. Note that although there are several steps, they do not necessarily need to be attempted in a linear order. That is, some components are independent of others. If you're struggling with one component and cannot complete it, then you may be able to skip over it and return to it later. Where outputs are needed in a subsequent step you may want to consider ways of side-stepping this need, e.g., by using Google translate output rather than your system output for translating the queries. You should aim to answer all components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "1ee8c73d7ca0e02aa8bdf2f7c3604894b452957e9698a3ad8b1b1182"
   },
   "source": [
    "## Information Retreival with BM25 (20% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "52109af14802e659ed495c16f053d7a1567ae514e75f358952c89b40"
   },
   "source": [
    "You should implement a vector-space retreival system using the BM25 approach. You'll need to:\n",
    " - load in the data files and tokenize the input,\n",
    " - preprocess the lexicon, e.g., by stemming and stop-word removal, \n",
    " - calculate the TF/IDF representation for all documents in the collection,\n",
    " - store an inverted index such that you can efficiently recover the documents matching a query term, and\n",
    " - implement querying in the BM25 model\n",
    " - test that it runs with some English queries (you'll have to make these up)\n",
    " \n",
    "This should run in a reasonable time over *dev.docs* (up to a few minutes to index), but beyond this does not need to be highly optimised. Feel free to use python dictionaries, sets, lists, numpy/scipy matrices etc where appropriate for best runtime, but you shouldn't need to use specialised data structures such as compression schemes or the like. Feel free to use APIs in NLTK, scikit-learn and other allowed python modules as needed (see list above.) You will probably want to save and load your index to/from disk to save repeated re-indexing every time you load the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "1ce6d886677eefbeb47de71d01ecac4a4ec3d18a219a46fd13bf3d89"
   },
   "outputs": [],
   "source": [
    "your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ee67e3c857d58f7ccb8ad50849b18dbf4190130da44eae4e1e6bb76a"
   },
   "source": [
    "```your supporting explanation and analysis goes here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f39ef10a4d0f4d7abf0d72f02ce80b37e2343261a90714aecac700e2"
   },
   "source": [
    "## Translating the queries (40% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "68b34873221842b09eaa34151a8c30d377a8f7f237b23d56d91d92e7"
   },
   "source": [
    "For translation, you should implement a simple word-based translation model in a noisy channel setting, where you search for the best translation string using a language model over the English with a translation model over the back-translation of the English output string, $\\vec{e}$, into the German input string, $\\vec{f}$. This corresponds to finding the string, $\\vec{e}$ which maximises $p(\\vec{e}) p(\\vec{f} | \\vec{e})$. This has two components:\n",
    "\n",
    "### Language model\n",
    "\n",
    "The first step is to estimate a language model. You should learn both a unigram language model and a trigram language model, and compare the two. Note that the unigram will be used in the *decoding* step below. You will have to think about how to smooth your estimates, and the related problem of handling unknown words. For smoothing the trigram model, you should you use backoff smoothing. There are a few choices to be made about how much probability mass to assign to the backoff, you will need to decide how to do this -- see the lecture slides and readings for some ideas. Be careful to pad the sentence with sentinel symbols to denote the start and end of each sentence such that the model can predict the words at the start of a sentence, and those when a sentence is complete.\n",
    "\n",
    "Please use *bitext-large.en* to train your models (start with *bitext-small.en* for your development) and evaluate the perplexity of your model on *newstest2013.en*. You should justify your development decisions and parameter settings in a markdown cell, and quantify their effect on perplexity, including the differences you notice between the unigram and the trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "93baa7a1aa69c753c09fde43f616d79d9cac26b72cb27d33b92c95a7"
   },
   "outputs": [],
   "source": [
    "your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2be8a89c0d2dae3586259f59b905a0a8d7e5eb84fc7ff3307b390538"
   },
   "source": [
    "### Translation model\n",
    "\n",
    "The next step is to estimate translation model probabilities. For this we will use a word-based alignment model, e.g., *ibm1* to learn word-based translation probabilities using expectation maxisation. Your task is to obtain good quality word alignments from this data, which will require careful use of the word alignment models. You will want to training in both translation directions, and combining the alignments.\n",
    "\n",
    "You should use the *bitext-small* files for this purpose, and be aware that it may take a minute or two to train ibm1 on this data. Inspect some of the word alignments and translation probabilities (e.g., using a few common words in German such as *haus*) to see if your approach is working, and give some examples of output alignments that you find that are good and bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "9b75c4309f9c7d5b6552ecdc51a72e6625c20226c91cb1d62a2ad4df"
   },
   "outputs": [],
   "source": [
    "your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "83610b49b936191a495f74a919d2a5eedb8dd63f8b1ab1d2456ae478"
   },
   "source": [
    "### Decoding \n",
    "\n",
    "Finally you'll need to solve for the best translation for a given string. For this you should do simple word-for-word translation where each word is translated independently. Use the alignments learned above (or the translation parameters) to translate each word of the queries into English. Compare taking the maximum probability of translation into English $p(e|f)$, with the noisy-channel probability, $p(f|e)p(e)$, which considers the reverse translation and a unigram language model probability. (You'll get a chance later to do something more ambitious, using the trigram model.)\n",
    "\n",
    "Use your algorithms to translate the first 100 queries into English, and save them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "e4d99fc912e92945a53d9d92accc09b97ebc54e400905bdae938dc7a"
   },
   "outputs": [],
   "source": [
    "your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c1ae60f068ca23d1b19c12046f6ff209c6e1ce4226ef61863937a4c1"
   },
   "source": [
    "Now you will need to write some text about the procedure above, and how well it worked. You should answer:\n",
    "  - how good are the translations? How do your methods fare at translating the content words versus functions words? You can use Google translate to provide an alternative (pretty good) translation for comparison purposes. \n",
    "  - what are good settings for various modelling parameters such as language model discounting, translation model smoothing, any decoding parameters, and how do these affect the outputs?\n",
    "  - compare the language model perplexity for the unigram and trigram models over your decoder outputs, how does the difference in perplexity between the two models compare to your test above on the monolingual data? \n",
    "  - how do the learned alignments differ between the two translation directions, and does their combination appear oto improve the predictions? (You don't need a formal evaluation here, as we have no *gold standard* alignments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "7b5250ed0054b8a1c3c4902535959a9319e19b51e6b922b068e39210"
   },
   "source": [
    "```your text goes here (markdown); aim to write a few paragraphs here.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e0e82aefc86e4956d099fb2a339d0989b057e2d4736ec41e2c253a9f"
   },
   "source": [
    "## Putting the CLIR system together (10% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "79548074730f23963d292589792ef47ba787d9b2fd6edfee425102b5"
   },
   "source": [
    "Now you should couple the translation and IR components from above. Take your translated queries and use these with the IR system to find the most relevant documents. You should then evaluate these predictions against the supplied relevance judgements, using the *mean average precision* (MAP) metric. As part of this, you will want to consider tuning the settings of the IR system for best performance, and comment on how successful your approach was and evaluate the IR performance is affected by the parameter settings and modelling decisions you have made.\n",
    "\n",
    "Note that if you were stuck on the translation steps above, or your translations are otherwise unusable, you can implement this step using google translate outputs for the 100 queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "signature": "8765525b2b8605ae1b60bd0f826b15d3fc4a8700988c67e42d4c8f13"
   },
   "outputs": [],
   "source": [
    "your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "f1a89be89fb5573dad2e2117e2751ec6cbb088f599f224c0478580a7"
   },
   "source": [
    "```your supporting analysis goes here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e9f0c27e82eadfa4d265e34f2352a4aa769256be69ee7a4796f8f48c"
   },
   "source": [
    "Note that in a complete CLIR you would translate the retreived documents back into German. We won't bother about that for this assignment, especially as I doubt many of you can understand German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "ee7f854342c24f4d93c21dd35f362af678c210c9de3f5f0a13b003e1"
   },
   "source": [
    "# Part 2: Extension and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6d0e9067b1e74bf7a7f22ba6f52c224f3aeeddef32370d743bb45143"
   },
   "source": [
    "## Extension (25% of assignment mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "b8dfadb67ebb7c70575e9685f5900d296c7bb53470d0b3d80c9c5929"
   },
   "source": [
    "You now have a working end-to-end CLIR system. The next step is to revisit some of the steps above to see if you can improve the performance of the system. You are invited to be creative with your ideas about how to do this. Note that many great ideas might not produce improvements, or end up being very difficult to implement. This will not be assessed solely on the grounds of CLIR accuracy; we will be looking primarily for interesting and creative ideas.\n",
    "\n",
    "Here are some ideas on what you might do. Warning: some are bigger than others, aim to spend 5-6 hours in total on this.\n",
    "\n",
    "#### Structured decoding\n",
    "\n",
    "Decode using a higher order language model, possibly with word reordering. Using your trigram language model and a word-based or phrase-based translation model, search for the best scoring translation sentence. You may want to use the tools in *nltk.translate*, including the phrase extraction and *StackDecoder*, especially if you wish to support word reordering. The decoding algorithm is much simpler (and tractable) without reordering, so you might want to implement this yourself. You will want to work with log-probabilities to avoid numerical underflow. Note  that the *StackDecoder* is not highly stable code, so you will need to understand its code in detail if you chose to use it. You will need to supply translation log probabilities and language model log probabilities as input to the decoder and consider the context of the previous two words in estimating the probability of each word. \n",
    "\n",
    "#### Probabilisitc querying\n",
    "\n",
    "Consider queries including weighted terms based on multiple translation outputs, such as the word translation probabilities or using *k-best* translations from a decoder (you will need to extract not just the best translation from the decode, but the runners up, which can be done from the stack contents after beam search.)  You should consider how best to define TF and DF for query words based on their ambiguous translations. You may find inspiration in the bibliography of [Douglas Oard](https://terpconnect.umd.edu/~oard/research.html#mlia) such as [this paper](https://terpconnect.umd.edu/~oard/pdf/coling12ture.pdf).\n",
    "\n",
    "#### Word alignment\n",
    "\n",
    "Implement an extended word alignment model, such as the *hmm* model (see [Vogel's paper](http://www.aclweb.org/anthology/C96-2141)) or a variant of *ibm2*. The variant of *ibm2* could include a better formulation for the distortion term, $p(i|j,I,J)$, based around rewarding values of i and j that are at similar relative positions to encourage alignments near the diagonal. This contrasts with *ibm2* which just learns a massive table of counts for all combinations of i,j,I and J. This strictly needs normalisation, as its not a probability, but for simplicity you can ignore this aspect here. This idea is inspired by [fast-align](https://github.com/clab/fast_align) which has a similar, but slightly more complex, formulation.\n",
    "\n",
    "You might also want to experiment with [fast-align](https://github.com/clab/fast_align) (an extremely fast and accurate variant of *ibm2*) or [GIZA++](http://www.statmt.org/moses/giza/GIZA++.html) (an implementation of *ibm1* - *ibm5* and the *hmm*), both widely used and highly optimised alignment tools. Neither of these tools are in python, and may be a little involved to install and compile.\n",
    "\n",
    "#### Decoding\n",
    "\n",
    "Experiment with a state-of-the-art translation tool like [Moses](http://www.statmt.org/moses/).  Can you get better CLIR performance using this, trained on the full parallel dataset? You might want to consider translating the document collection into German, and then doing the IR entirely in German. You might consider building a python interface to moses, building on the existing very basic funcationality. **Warning:** Moses is a complex suite of software, and can be a little complex to install and use. However there are good installation and usage guides on the moses webpage.\n",
    "\n",
    "#### IR engine\n",
    "\n",
    "There are several great open source IR engines, including [Lucene](https://lucene.apache.org/core/), [Terrier](http://terrier.org/), [Zettair](http://www.seg.rmit.edu.au/zettair/) and [Lemur](http://www.lemurproject.org/). You could experiment with these, and use this to compare the accuracy of different types of IR models on the CLIR data. Alternatively, you could implement a faster or more compact retreival system in python, e.g., using compressed vbyte encodings, skip lists or similar.\n",
    "\n",
    "Note that the development of some of the extensions maye require working in the command shell and in other languages than python. If this is the case, you should attach your code in separate files as part of your submission, but include the analysis text here. *Note that excellent submissions implementing interfaces or models not currently in NLTK will be considered for inclusion into the toolkit.* You should include both the code, and text explaining your work and findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "signature": "48aefa5acbd6e15c2ab02bf5fbbedceee6ba2ce7d5e2cbe32269c8cf"
   },
   "outputs": [],
   "source": [
    "your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "55f2dd376df2bc20593a672779dc0b27802327259990b716378f973b"
   },
   "source": [
    "```your supporting description analysis goes here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "61f6017e3a187480874c49498f85e5d43ba9f0ba2bdb8eb4ed08300e"
   },
   "source": [
    "## Discussion (5% of marks)\n",
    "\n",
    "What conclusions can you make about CLIR, or more generally translation and IR? Overall what approaches worked and what ones didn't? What insights do you have from doing this, put another way, if you were to start again, what approach would you take and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "95059a348959b5816025d842e756178f96e9094761067e3883a4ad59"
   },
   "source": [
    "```your analysis text goes here; aim to write a paragraph or two```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
