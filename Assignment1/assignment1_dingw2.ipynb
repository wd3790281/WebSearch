{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "284b97c106ad3f0bbc14119eb1cbaebd4c23cc25f850f4356e6a4287"
   },
   "source": [
    "# COMP90042 Assignment #1: Sentiment analysis for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "signature": "2b34778b3e7a11eb1806238722f72098c253bb77c9938b01a354c69d"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-00491777b799>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-00491777b799>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Student Name: Ding Wang\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Student Name: Ding Wang\n",
    "Student ID: 722822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a4d148693b1feecbdb1e4b3fca73cee05cd1d204412e13335e2c0486"
   },
   "source": [
    "## General info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "7e7157a8860c67083d5f0da50ede7705bd076cfd98bd5b83dc1320b9"
   },
   "source": [
    "<b>Due date</b>: 5pm, Mon April 12\n",
    "\n",
    "<b>Submission method</b>: see LMS\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this ipython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day, no late submissions after the first week\n",
    "\n",
    "<b>Marks</b>: 25% of mark for class\n",
    "\n",
    "<b>Overview</b>: For this project, you'll be building a 3-way polarity classification system for tweets, using a logistic regression classifier, BOW features, as well as polarity lexicons built from external sources. A key focus of this project is critical analysis and experimental evaluation, for which you will need to report on the relative merits of various options. \n",
    "\n",
    "<b>Materials</b>: See the main class LMS page for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Sci-kit Learn, and Gemsim. In particular, if you are not using a lab computer which already has it installed, we recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python build-in packages, but do not use any other 3rd party packages; if your iPython notebook doesn't run on the marker's machine, you will lose marks. You are encouraged to use the iPython notebooks released for this class as well as other online documentation to guide your responses, but you should not copy directly from any source. The only other data you will need is three sets of tagged tweets, the first two of which (training and dev) were released at the same time as this notebook, and a third set (test) which will be made available about a week before the assignment is due, see Final Testing below. This data comes from the recent SemEval 2016 shared task. Do not distribute this data indiscriminately (i.e. put it on a public website), you should use it only for this assignment, and delete it afterwards. The corpus is comprised of unfiltered text from the web, and may include offensive or objectionable material. This reflects the general composition of the web and the general challenges present in web based text analysis. The University of Melbourne takes no responsibility for opinions expressed in the corpus, nor takes any responsibility for offence caused by these documents.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time (less than 10 minutes on a lab desktop), and you must follow all instructions provided below, including specific implementation requirements. You will be marked not only on the correctness of your methods, but also on your explanation and analysis. Please do not change any of instruction text in the notebook. Where applicable, leave the output cells in the code, particularly when you are commenting on that output. You should add your answers and code by inserting a markdown cell between every major function or other block of code explaining its purpose or anywhere a result needs to be discussed (see the class notebooks for examples). Note that even if you do something wrong, you might get partial credit if you explain it enough that we can follow your reasoning, whereas a fully correct assignment with no text commentary will not receive a passing score. You will not be marked directly on the performance of your final classifier, but each of the steps you take to build it should be reasonable and well-defended.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via LMS. Minor changes and clarifications will be announced in the forum on LMS, we recommend you check the forum regularly.\n",
    "\n",
    "<b>Academic Misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this project, and we encourage you to discuss it in general terms with other students. However, it is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2f9cb9dd5aeba77b46f6249edb71bb26cc4b5fc3761d1ff271bfa821"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "signature": "cb3de4c667616ed854e7aae225551bfaa3095ce8daf2a50a07aefd64"
   },
   "source": [
    "<b>Instructions</b>: Your first task is to carry out preprocessing on the tweets. Use the code below as a starter. Each line of the input files is a json including the tweet and the label (and the tweet id), this code just loads them into a list without any preprocessing. Note that for the labels, 1 = positive, 0 = neutral, -1 = negative. Here is a list of things your preprocessing code must do:\n",
    "\n",
    "<ul>\n",
    "<li>Segment into sentences: Use NLTK punkt sentence segmenter</li>\n",
    "<li>Tokenize sentences: Use the NLTK regex WordPunct tokenizer</li>\n",
    "<li>Lowercase all words</li>\n",
    "<li>Remove Twitter usernames: Usernames on twitter begin with @</li>\n",
    "<li>Remove URLs: URLs start with http</li> \n",
    "<li>Remove any hashtags from their original location in the tweet, tokenize them, and add them as a separate sentences with the hash tag removed: for tokenization, use capitalized letters when they occur (e.g. #RefugeesWelcome -> Refugees Welcome), or when there is no capitalization (#refugeeswelcome -> refugees welcome) use the MaxMatch algorithm and the list of English words included in NLTK (nltk.corpus.words.words()). Two notes about the English word list: 1. you should convert it to a python set before you use it (sets are hashed, so you get much quicker lookup) 2. It contains only base forms, so you will need to lemmatize words before you look them up.</li>\n",
    "</ul>\n",
    "\n",
    "You can do these in almost any order you like, but it may be useful to do the main segmentation/tokenization last (or almost last), since for the other tasks it is easier to deal with the raw string rather than a list of tokens. The use of regular expressions is recommended, but not required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "signature": "73ef1f5b385207c255d3de3f702dd6909dbb37e0e77494ea81fd04fe"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "words_set = set(words.words())\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords_set = set(stopwords)  \n",
    "\n",
    "def preprocess(tweet):\n",
    "    processedTweet = tweet\n",
    "    \n",
    "    #remove all TweeterUserNames start with @\n",
    "    processedTweet = re.sub(r'@\\w*','',processedTweet)\n",
    "    \n",
    "    #remove url\n",
    "    processedTweet = re.sub(r'https?\\S*','',processedTweet)\n",
    "    \n",
    "    #remove all hashtags and seperate words after hashtags\n",
    "    hashtags_with_words = re.findall(r'#\\w+',processedTweet)\n",
    "    processedTweet = re.sub(r'#\\w+', \" \", processedTweet)\n",
    "    seperated_words = []\n",
    "    for hashtag_word in hashtags_with_words:\n",
    "        hashtag_word = hashtag_word.replace('#','')\n",
    "        hashtag_word = hashtag_word.lower()\n",
    "        seperated_words += maxMatch(seperated_words, hashtag_word)\n",
    "   \n",
    "    #turn every word to lowercase\n",
    "    processedTweet = processedTweet.lower()\n",
    "    \n",
    "    #segment into sentences\n",
    "    sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    processedTweet = sent_segmenter.tokenize(processedTweet)\n",
    "    \n",
    "    tokens = []\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    for sentence in processedTweet:\n",
    "        tokens += tokenizer.tokenize(sentence)\n",
    "    \n",
    "    tokens = tokens + seperated_words\n",
    "    \n",
    "    #lemmatiz\n",
    "    lemmas = []\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token,'v')\n",
    "        if lemma == token:\n",
    "            token = lemmatizer.lemmatize(token,'n')\n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    return lemmas\n",
    "\n",
    "def maxMatch(wordlist,sentence):\n",
    "    i = len(sentence)\n",
    "    \n",
    "    while(i != 0):\n",
    "        firstword = sentence[0:i]\n",
    "        remainder = sentence[i:(len(sentence))]\n",
    "        lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lemma = lemmatizer.lemmatize(firstword,'v')\n",
    "        if lemma == firstword:\n",
    "            firstword = lemmatizer.lemmatize(firstword,'n')\n",
    "        if firstword in words_set:\n",
    "            i = i-1\n",
    "            wordlist.append(firstword)\n",
    "            return maxMatch(wordlist, remainder)          \n",
    "        else :\n",
    "            i = i-1\n",
    "            continue\n",
    "    return wordlist\n",
    "    \n",
    "\n",
    "def preprocess_file(filename, ):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        tweets.append(preprocess(tweet_dict[\"text\"]))\n",
    "        labels.append(int(tweet_dict[\"label\"]))\n",
    "    return tweets, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "343aafa9749f7a7132c7cea1fa706947f6c9dc376dd41601217247f6"
   },
   "source": [
    "<b>Instructions</b>: Once your basic preprocessing module is working, run it on the training set and have it print out 10 examples where your system identified a hashtag with more than one word inside; print out both the original tweet string as well as result after preprocessing. It's okay if you have to duplicate some code from above to do this. Point out any errors you see in the preprocessing, and discuss possible solutions; these can be related to the hashtags, or any other errors you see. You do not have to fix the errors unless they actually indicate a actual bug in your code (at which point you should go back to the previous section, fix the code, and print out the samples again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon. ---- [u'dear', u'the', u'newooffice', u'for', u'mac', u'be', u'great', u'and', u'all', u'but', u'no', u'lync', u'update', u'c', u'mon']\n",
      "@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it! ---- [u'how', u'about', u'you', u'make', u'a', u'system', u'that', u'doesn', u't', u'eat', u'my', u'friggin', u'discs', u'this', u'be', u'the', u'2nd', u'time', u'this', u'have', u'happen', u'and', u'i', u'be', u'so', u'sick', u'of', u'it']\n",
      "If I make a game as a #windows10 Universal App. Will #xboxone owners be able to download and play it in November? @majornelson @Microsoft ---- [u'if', u'i', u'make', u'a', u'game', u'as', u'a', u'universal', u'app', u'will', u'owners', u'be', u'able', u'to', u'download', u'and', u'play', u'it', u'in', u'november', u'window', u'window', u'x', u'box', u'one', u'window', u'window', u'x', u'box', u'one']\n",
      "Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft ---- [u'microsoft', u'i', u'may', u'not', u'prefer', u'your', u'game', u'branch', u'of', u'business', u'but', u'you', u'do', u'make', u'a', u'damn', u'fine', u'operate', u'system', u'window', u'window']\n",
      "@MikeWolf1980 @Microsoft I will be downgrading and let #Windows10 be out for almost the 1st yr b4 trying it again. #Windows10fail ---- [u'i', u'will', u'be', u'downgrade', u'and', u'let', u'be', u'out', u'for', u'almost', u'the', u'1st', u'yr', u'b4', u'try', u'it', u'again', u'window', u'window', u'window', u'window', u'window', u'window']\n",
      "@Microsoft 2nd computer with same error!!! #Windows10fail Guess we will shelve this until SP1! http://t.co/QCcHlKuy8Q ---- [u'2nd', u'computer', u'with', u'same', u'error', u'guess', u'we', u'will', u'shelve', u'this', u'until', u'sp1', u'window', u'window']\n",
      "Just ordered my 1st ever tablet; @Microsoft Surface Pro 3, i7/8GB 512GB SSD. Hopefully it works out for dev to replace my laptop =) ---- [u'just', u'order', u'my', u'1st', u'ever', u'tablet', u'surface', u'pro', u'3', u'i7', u'8gb', u'512gb', u'ssd', u'hopefully', u'it', u'work', u'out', u'for', u'dev', u'to', u'replace', u'my', u'laptop']\n",
      "After attempting a reinstall, it still bricks, says, \"Windows cannot finish installing,\" or somesuch. @Microsoft may have cost me $600. ---- [u'after', u'attempt', u'a', u'reinstall', u'it', u'still', u'bricks', u'say', u'windows', u'cannot', u'finish', u'instal', u'or', u'somesuch', u'may', u'have', u'cost', u'me', u'600']\n",
      "Sunday morning, quiet day so time to welcome in #Windows10 @Microsoft @Windows http://t.co/7VtvAzhWmV ---- [u'sunday', u'morning', u'quiet', u'day', u'so', u'time', u'to', u'welcome', u'in', u'window', u'window']\n",
      "Did @Microsoft break Windows 10? Was working fine on Wednesday but now I can't get passed the login screen without it freezing up. ---- [u'do', u'break', u'windows', u'10', u'be', u'work', u'fine', u'on', u'wednesday', u'but', u'now', u'i', u'can', u't', u'get', u'pass', u'the', u'login', u'screen', u'without', u'it', u'freeze', u'up']\n"
     ]
    }
   ],
   "source": [
    "train_tweets, labels = preprocess_file(u'train.json')\n",
    "otweets = []\n",
    "f = open(u'train.json')\n",
    "for line in f:\n",
    "    tweet_dict = json.loads(line)\n",
    "    otweets.append(tweet_dict[\"text\"])\n",
    "for i in range(0,10):\n",
    "    print otweets[i], \"----\", train_tweets[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed errors:\n",
    "<b>1.</b>  The preprocess progress cannot solve the abbreviation problem, such as \"dosen't\" is divided to \"doesn\" and \"t\", it doesn't return to the original format of the word.\n",
    "\n",
    "\n",
    "<b>2.</b>  Plural word is divied. \"windows\" is divided to \"window\" and \"s\". \"s\" here is meaningless. It should be removed. It will be a meaningless feature in the dictionary below.\n",
    "\n",
    "\n",
    "<b>3.</b>  Words after hashtags may be over divided due to the limit word dictionary used in max-mathc algorithm. For example, \"xboxone\" is divided to \"x\", \"box\" and \"one\". The meaning of the original word is totally disappeared after division. This will really effect the building of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4a3041d3dae858daf24e712eb38ef7c69d9a5e1b5c2a61c093fc3a29"
   },
   "source": [
    "<b>Instructions</b>: The next step will be to convert each of your preprocessed tweets into a feature dictionary, that is, a python dictionary where each entry corresponds to a feature and its value. At this stage, you should just build a bag-of-word feature dict, though you must allow for two possible options: one is to remove stopwords (using the NLTK stopword list), and the other is to remove words appearing <em>less</em> than n times across the entire training set (n<=0 should have no effect). The outer function (convert_to_feature dicts) should take the list of tweets resulting from the preprocess_file, and return a list of feature dictionaries in the same order (so they correspond to the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "signature": "30176ade1db34e2564313e9e3b552e703b8cab4f850a59e016d2a410"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# this cell may run for a several minutes to convert the dictionary\n",
    "def convert_to_feature_dicts(tweets,remove_stop_words,n):\n",
    "    # if require to remove the words appear times less than n (n>0), find the words and store it in a list\n",
    "    # if n == 0, words is an empty list\n",
    "    if n!= 0:\n",
    "        words = less_than_n(tweets, n)\n",
    "    else:\n",
    "        words = []\n",
    "    feature_dicts = [] \n",
    "    tokens = []\n",
    "    for tweet in tweets:     \n",
    "        # if require to remove stop words, remove all the stop words from the tweet         \n",
    "        if (remove_stop_words):\n",
    "            tokens = remove_stopwords(tweet)     \n",
    "        else:\n",
    "            tokens = tweet        \n",
    "        counter = Counter()        \n",
    "        # check whether the token is in words to be removed\n",
    "        for token in tokens:\n",
    "            if token not in words:\n",
    "                counter[token] += 1\n",
    "        feature_dict = dict(counter)       \n",
    "        feature_dicts.append(feature_dict)\n",
    "    return feature_dicts\n",
    "\n",
    "# this function is to find all the words appearance less than n times, return a list\n",
    "def less_than_n(tweets, n):\n",
    "    wcounter = Counter()\n",
    "    to_be_removed = []\n",
    "    for tweet in tweets:\n",
    "        wcounter += Counter(tweet)    \n",
    "    for ele, count in dict(wcounter).iteritems():\n",
    "        if wcounter[ele] <= n:\n",
    "            to_be_removed.append(ele)\n",
    "    return to_be_removed\n",
    "\n",
    "# this function is to remove stop words from a list of tokens\n",
    "def remove_stopwords (tokens):\n",
    "    \n",
    "    words_without_stopwords = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords_set:\n",
    "            words_without_stopwords.append(token)\n",
    "    return words_without_stopwords\n",
    "\n",
    "trainset, trainlabel = preprocess_file(u'train.json')\n",
    "devset, devlabel = preprocess_file(u'dev.json')\n",
    "\n",
    "train_dict = convert_to_feature_dicts(trainset,1,1)\n",
    "dev_dict = convert_to_feature_dicts(devset,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4e39719495db1c19c5c14d4606425272fa9b27b6b778522a4a8f44ce"
   },
   "source": [
    "## Tuning and classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "eb81580f49dd36f01be4f901e75c6dfee49f1baaa105d9e0f07db715"
   },
   "source": [
    "<b>Instructions</b>: Using the functions you've written, you should produce lists of feature dictionaries for both training and development sets; for the training set, remove stopwords and all words that appear only once (do <em>not</em> this for the dev set). Using scikit learn, convert the data to the sparse representation used for training classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "signature": "97c9d959bee45da5f27d5a64b0603629277ac2420d6925ecd532d680"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# build sparse metrics\n",
    "def prepare_for_classification (traindict, devdict):    \n",
    "    \n",
    "    vectorizer = DictVectorizer()\n",
    "    traindata = vectorizer.fit_transform(traindict)\n",
    "    devdata = vectorizer.transform(devdict)\n",
    "    \n",
    "    return traindata, devdata\n",
    "\n",
    "train_data, dev_data = prepare_for_classification(train_dict,dev_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: Now, tune a decision tree classifier using accuracy in the development set as the evaluation metric. For this, you need to consider at least 2 parameters of the model likely to influence performance and which make sense in this context; you should read the documentation for the classifier on sci-kit learn website to learn what these parameters are. For any binary or categorical parameters, you should just consider all options. For numerical values, you should start by keep other settings on default and just randomly try a wide range, looking for values above which there is a steep drop-off in performance, or, alternatively, no effect on performance at all (you don't need to show this process in the notebook). Remember that some parameters should be tested on a logarithmic scale. Once you're fairly confident of a good range for the parameter, divide it up into at least 5 steps (but no more than 10), and carry out a grid search, which is to say an exhaustive exploration of all parameter options within the limits you've set (this should be included in the notebook). Identify the best parameter values, and discuss the influence of the parameters on performance in the development set. Do you think some values of the parameters are resulting in overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "signature": "8b2ba6b91a62c0b26f1aff9c5f326687c2ba3f6a29bd5227db64c397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini 2 best 0.457080371788\n",
      "gini 2 random 0.4581738655\n",
      "gini 3 best 0.459814106069\n",
      "gini 3 random 0.472936030618\n",
      "gini 4 best 0.464734827775\n",
      "gini 4 random 0.47402952433\n",
      "entropy 2 best 0.465828321487\n",
      "entropy 2 random 0.450519409513\n",
      "entropy 3 best 0.468562055768\n",
      "entropy 3 random 0.463641334062\n",
      "entropy 4 best 0.471842536905\n",
      "entropy 4 random 0.461454346638\n",
      "('gini', 4, 'random') 0.47402952433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import operator\n",
    "\n",
    "def build_classifer():\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(train_data, trainlabel)\n",
    "    return clf\n",
    "\n",
    "#  tune 3 parameters in Decision tree, give the value of the params, return the accuracy\n",
    "def tune_dt(c, s, sp):\n",
    "    dt = DecisionTreeClassifier(criterion= c, min_samples_leaf= s, splitter = sp)\n",
    "    dt.fit(train_data, trainlabel)\n",
    "    accurancy = dt.score(dev_data,devlabel)\n",
    "    return accurancy\n",
    "\n",
    "dtparas = []\n",
    "dtscores = []\n",
    "for c in ['gini', 'entropy']:\n",
    "    for s in [2, 3,4]:\n",
    "        for sp in ['best', 'random']:\n",
    "            score = tune_dt(c,s,sp)\n",
    "            dtscores.append(score)\n",
    "            dtparas.append((c,s,sp))\n",
    "            print c,s,sp, score\n",
    "\n",
    "# find out the best combination of the parameters and give out the accuracy in dev set\n",
    "dtindex, dtvalue = max(enumerate(dtscores), key=operator.itemgetter(1))\n",
    "dtbest = dtparas[dtindex]\n",
    "print dtbest, dtvalue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "The best value for parameter criterion,  min_samples_leaf and splitter are \"gini, 4 , best\". Criterion measures the quality of a split, min_sample_leaf effect the minimal samples looked for, splitter is the strategy used to choose the split at each node. Splitter \"best\" may result in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "42630d701b5a1ec8c0903fe0571f11f7dec58fc763ddee0d38ff09a7"
   },
   "source": [
    "<b>Instructions</b>: Carry out the same tuning process with the logistic regression classifier. Compare the performance of the two classifiers to each other, and to the most common class baseline. How are the classifiers doing? Is this a challenging task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "signature": "0122e357dd6cd500b17f88f21dab3e9089bee2d5d55e0a9ee79830f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('liblinear', True) 0.504647348278\n",
      "0.382722799344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression    \n",
    "from sklearn.dummy import DummyClassifier\n",
    "import operator\n",
    "\n",
    "def build_lg():\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_data, trainlabel)\n",
    "    return clf\n",
    "\n",
    "def tune_lg(s, f):\n",
    "    c = LogisticRegression(solver= s, fit_intercept = f)\n",
    "    c.fit(train_data, trainlabel)\n",
    "    accurancy = c.score(dev_data,devlabel)\n",
    "    return accurancy\n",
    "\n",
    "paras = []\n",
    "scores = []\n",
    "for s in ['newton-cg','lbfgs', 'sag', 'liblinear']:\n",
    "    for f in [True, False]:\n",
    "        scores.append(tune_lg(s,f))\n",
    "        paras.append((s,f))\n",
    "\n",
    "index, value = max(enumerate(scores), key=operator.itemgetter(1))\n",
    "best = paras[index]\n",
    "print best, value\n",
    "\n",
    "base = DummyClassifier(strategy='most_frequent')\n",
    "base.fit(train_data, trainlabel)\n",
    "baseline = base.score(dev_data, devlabel)\n",
    "print baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "Baseline is around 0.3827, decision tree is around 0.47, logistic regression is around 0.5. So logistic regression performs better. This is a challenge task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "c914fda20b4566e632f11722cbaae4f53000cf9aa14a31cef6898e04"
   },
   "source": [
    "<b>Instructions</b>: The next task is a slight detour to test your understanding of the logistic regression classifier: you are going to build your own classifier based on the trained model from sci-kit learn. In particular, you should fill in the MyLogisticRegression class started below which is initialized using the feature weights (coefficients) and constants (intercepts) and list of labels (classes) from the sci-kit learn classifier (see the \"Attributes\" in the documentation for the Logistic Regression classifier), and which mimics the predict and predict_proba methods from the sci-kit learn classifier object. You should confirm that your solution works by using it in the task at hand: take the classifier defined below, train it on the training data, then create an instance of MyLogisticRegression, and show that your classifier has the same output as the scikit-learn classifier for both predict and predict_proba for 5 samples from the development set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "signature": "65eec0a60a1866c27e03ff2b9bb5dcbd83b4e025934cc75e5775e923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, -1, 1]\n",
      "[ 0  0  1 -1  1]\n",
      "[[0.3565083059101238, 0.5499135639348658, 0.09357813015501039], [0.001980955095881981, 0.7135278867616557, 0.2844911581424623], [0.1504821889153988, 0.08276582911182184, 0.7667519819727794], [0.44564178864544557, 0.41041594945192256, 0.14394226190263185], [0.10284847710656625, 0.26244551174640396, 0.6347060111470298]]\n",
      "[[ 0.35650831  0.54991356  0.09357813]\n",
      " [ 0.00198096  0.71352789  0.28449116]\n",
      " [ 0.15048219  0.08276583  0.76675198]\n",
      " [ 0.44564179  0.41041595  0.14394226]\n",
      " [ 0.10284848  0.26244551  0.63470601]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, weights, constants, labels):\n",
    "        self.weights = weights\n",
    "        self.constants = constants\n",
    "        self.labels = labels\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "       \n",
    "        rownum = X.shape[0]\n",
    "        classnum = self.labels.shape[0]\n",
    "        all_results = []\n",
    "        for i in range(rownum):\n",
    "            denom = 0.0\n",
    "            row_product = []\n",
    "            result = []\n",
    "            for j in range(classnum):\n",
    "                v = math.exp(X[i].dot(self.weights[j])[0] + self.constants[j])\n",
    "                row_product.append(v)\n",
    "                denom += v\n",
    "            for rp in row_product:\n",
    "                proba_result = rp/denom\n",
    "                result.append(proba_result)\n",
    "            all_results.append(result)\n",
    "        return all_results\n",
    "    \n",
    "    def predict(self,X):\n",
    "        answer = []\n",
    "        probabilities = self.predict_proba(X)\n",
    "        for probability in probabilities:\n",
    "            maxindex, maxvalue = max(enumerate(probability), key=operator.itemgetter(1))\n",
    "            answer.append(self.labels[maxindex])\n",
    "        return answer\n",
    "     \n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "clf.fit(train_data, trainlabel)\n",
    "\n",
    "my_clf = MyLogisticRegression(clf.coef_, clf.intercept_, clf.classes_)\n",
    "x = my_clf.predict(dev_data[:5])\n",
    "print x\n",
    "x = clf.predict(dev_data[:5])\n",
    "print x\n",
    "\n",
    "x = my_clf.predict_proba(dev_data[:5])\n",
    "print x\n",
    "x = clf.predict_proba(dev_data[:5])\n",
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "a6d50f49ecc05053c34c47563849fe136f7ce685e6ec097ca17ed926"
   },
   "source": [
    "## Polarity Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "8073a77de40ef277dfa3e60ec41487e0e62f8909a275ab7e92a007e4"
   },
   "source": [
    "<b>Instructions</b>: Next we will try integrating information from sources beyond the training set, in the form of polarity lexicons. The main focus of this section is producing and evaluating 3 automatically-built polarity lexicons. The first of these lexicons is SentiWordNet, which is <a href=\"http://www.nltk.org/howto/sentiwordnet.html\"> accessible through NLTK</a>. SentiWordNet has precalculated scores for positive, negative, and neutral sentiment for some of the words in WordNet, but, like WordNet, it is arranged in synsets; building a WSD system to handle this is beyond the scope of this assignment, instead you should take the most common polarity across its senses (neutral if there is a tie). Do this by iterating through all the synsets in WordNet (which may take a little while, the code snippet below has a counter to show your progress), and then create two lists, one of positive words, one of negative words. Show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "28ea5c3a268cfb39723fc7f1c4bc72626dbf5f31484c6fc10bc351a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'comically', u'spotty', u'couthie', u'in_good_taste', u'fit']\n",
      "[u'screaming', u'grueling', u'second_childhood', u'inanimate', u'unsinkable']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "\n",
    "def get_polarity_type(synset_name):\n",
    "    swn_synset =  swn.senti_synset(synset_name)\n",
    "    if not swn_synset:\n",
    "        return None\n",
    "    elif swn_synset.pos_score() > swn_synset.neg_score() and swn_synset.pos_score() > swn_synset.obj_score():\n",
    "        return 1\n",
    "    elif swn_synset.neg_score() > swn_synset.pos_score() and swn_synset.neg_score() > swn_synset.obj_score():\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "positive1 = []\n",
    "negative1 = []\n",
    "lemmas_dict = {}\n",
    "for synset in wn.all_synsets():\n",
    "#     count += 1\n",
    "#     if count % 1000 == 0:\n",
    "#         print count\n",
    "    # count synset polarity for each lemma\n",
    "    name = synset.name()\n",
    "    result = get_polarity_type(name)\n",
    "    lemmas = synset.lemma_names()\n",
    "    for lemma in lemmas:\n",
    "        lemmas_dict[lemma] = lemmas_dict.get(lemma, 0) + result\n",
    "        \n",
    "for key, value in lemmas_dict.iteritems():\n",
    "    if value > 0:\n",
    "        positive1.append(key)\n",
    "    elif value < 0:\n",
    "        negative1.append(key)\n",
    "        \n",
    "print positive1[0:5]\n",
    "print negative1[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "4e2027a4b0ca09747bf7b8d39a5c4f03abcab0434533af6d97d24451"
   },
   "source": [
    "<b>Instructions</b>: The second lexicon will be built using the word2vec (CBOW) vectors included in NLTK. For this, you will need a small set of positive and negative seed terms, which are given to you below. Calculate cosine similarity between vectors of the seeds terms and each of the words for which you have vectors (if you use Gensim, you can iterate over model.vocab), flip the sign for the negative seeds, and then average to get a score. Use this score to produce a list of positive and negative words; you should include a threshold of ±0.03 for words to be considered positive or negative. Again, show 5 examples of each of the positive and negative words, and comment on their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "signature": "aa8daa1683937fe9f7633f6b3b90192a8dd9ef11081d950a77b83d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.624682116862\n",
      "[u'loen', u'pergamon', u'feasibility', u'pampa', u'modest']\n",
      "[u'debts', u'clotted', u'hastily', u'comically', u'disobeying']\n"
     ]
    }
   ],
   "source": [
    "positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "import gensim \n",
    "import numpy\n",
    "\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "big_model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "\n",
    "twoseeds_simi = big_model.n_similarity(positive_seeds, negative_seeds)\n",
    "\n",
    "positive2 = [] \n",
    "negative2 = []\n",
    "\n",
    "psize = len(positive_seeds)\n",
    "nsize = len(negative_seeds)\n",
    "\n",
    "for word in big_model.vocab:\n",
    "    simi_sum = 0\n",
    "    # compute the sum of the word to every seed in both positive_seeds and negative_seeds\n",
    "    try:\n",
    "        for pseed in positive_seeds:\n",
    "            simi_sum += big_model.similarity(word, pseed)\n",
    "        for nseed in negative_seeds:\n",
    "            simi_sum -= big_model.similarity(word, nseed)\n",
    "    except Exception, e:\n",
    "        print e\n",
    "    score = simi_sum/(psize+nsize)\n",
    "    if score > 0.03:\n",
    "        positive2.append(word.lower())\n",
    "    elif score < -0.03:\n",
    "        negative2.append(word.lower())\n",
    "                          \n",
    "print twoseeds_simi\n",
    "print positive2[:5] \n",
    "print negative2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "15e7fc419322f37252e6e69b70a08ce905ae0168a782d16d4393d6ec"
   },
   "source": [
    "<b>Instructions</b>: The third lexicon will be built by calculating PPMI with the seed terms. For this, use the Brown corpus included in NLTK, with co-occurrence defined as <em>binary</em> text co-occurrence (that is, multiple co-occurrences in the same text are not counted); importantly, your solution should <em>not</em> calculate the entire co-occurrence matrix, since you only care about relative co-occurrence with the seeds. As above, average the resulting similarity scores after switching the sign for the negative seeds and use them to produce a list of positive and negative words, and check 5 of each. For PPMI, use a threshold of  ±0.3 for deciding if a word is neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "signature": "267b4e44960ea0ab65d5aca0366dd7779114f0719fbc139fab2e6cb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'gontran', u'hats', u\"detective's\", u'four', u'facilities', u'appetite', u'whose', u'electricity', u'seriously', u'presents', u'under', u'lord', u'sorry', u'pride', u'flushing', u'misunderstanding', u'vastly', u\"we'll\", u'crops', u'cause', u'companies', u'remoteness', u'graces', u'clothes', u'force', u'specially', u'guidelines', u'estimates', u'japanese', u'approximation', u'second', u'even', u'christ', u'rockies', u'orchestra', u'outfielders', u'new', u'net', u'non-violence', u'inducement', u'men', u'met', u'obtained', u'voice', u'study', u'absorbency', u'advantages', u\"i'm\", u'changes', u'criticism', u'shortened', u'explained', u'infield', u'feelings', u'brought', u'moral', u'total', u'would', u'hospital', u'negative', u'music', u'passport', u'type', u'tell', u'preconditioned', u'successful', u'wars', u'aware', u'excellent', u'hold', u'me', u'word', u'work', u'groves', u'era', u'mechanic', u'exceptions', u'my', u'example', u'squirting', u'indicated', u'give', u'ashikaga', u'involve', u'caution', u'want', u'present-day', u'end', u'turn', u'provide', u'debauchery', u'how', u'writers', u'answer', u'elizabeth', u'beauty', u'after', u'wrong', u'president', u'misleads', u'attempt', u'appreciate', u'greek', u'complexity', u'green', u'ultimate', u'democratic', u'order', u'wine', u'interpretation', u'feedback', u'fall', u'mayor', u'before', u'fit', u'personal', u'expectations', u'2:06', u'writing', u'better', u'distinguishing', u\"south's\", u'overcome', u'then', u'them', u'combination', u'weakness', u'safe', u'they', u'schools', u'glazed', u'one', u'decorator', u'meat', u'reasonably', u'slavery', u'victory', u\"doctor's\", u'went', u'meal', u'bone', u'lasting', u'observance', u'series', u'used', u'chords', u'cathedral', u'whit', u'ring', u'network', u'trumpeter', u'god', u'particles', u'fellowship', u'laid', u'surprise', u'engineer', u'negociant', u'struggle', u'mothers-in-law', u'created', u'cosmic', u'days', u'priorities', u'creates', u'kingdom', u'purpose', u'already', u'features', u'grade', u'another', u'navigator', u'clarity', u'awfully', u'service', u'too', u'percentage', u'john', u'urban', u'murder', u'g.o.p.', u'took', u'wisdom', u'gambles', u'distance', u'target', u'showed', u'tree', u'nations', u'project', u'matter', u'historical', u'feeling', u'powers', u'ran', u'modern', u'mind', u'all-time', u'sportsmen', u'seen', u'thoroughly', u'hamper', u'circles', u'though', u'exercised', u'involving', u'mouth', u'letter', u'doo', u'professor', u'spend', u'memberships', u'competitors', u'reputation', u'enterprise', u'nineteenth', u'scream', u'came', u'decidedly', u'pope', u'ambassador', u'queen', u'chandelier', u'radio', u'bemaddening', u'enjoyed', u'busy', u'resemblance', u'consciousness', u'explain', u'rich', u'folks', u'do', u'yff', u'mixture', u'dr.', u'coast', u'despite', u'report', u'nasty', u'tribute', u'runs', u'twice', u'bad', u'stead', u'ruins', u'habit', u'observed', u'result', u'said', u'pressures', u\"there's\", u'cooperation', u'we', u'never', u'nature', u'however', u'boss', u'lefthanders', u'extent', u'contamination', u'news', u\"college's\", u'horsemen', u'goddess', u'cow', u'country', u'adventures', u'linguist-anthropologist', u'planned', u'distinction', u'expense', u'confronted', u'regarded', u'character', u'springs', u'binge', u'three', u'been', u'.', u'much', u'interest', u'warmth', u'life', u'quantities', u'worker', u'enterprises', u'save', u'hat', u'ugly', u'teachers', u'aid', u'balance', u'smoky', u'remembering', u'played', u'is', u'marksmanship', u'it', u'conditioned', u'in', u'motivating', u'stint', u'if', u'1815', u'containing', u'things', u'make', u'airport', u'patient', u'gateways', u'unfortunate', u'advances', u'several', u'european', u'fairly', u'catholic', u'published', u'qualifications', u'evil', u'delight', u'characters', u'potentials', u'blamed', u'kid', u'thy', u\"mother's\", u'possessed', u'suffering', u'greatest', u'qualities', u'mother', u'hitters', u'the', u'musical', u'seasonal', u'just', u'out-of-doors', u'antiseptic', u'satisfactions', u'had', u'mr.', u'cognac', u'easy', u'has', u'espoused', u'gave', u'maye', u'dignity', u'elders', u'bastards', u'judge', u'cape', u'dreams', u'psychological', u'offices', u'officer', u'night', u'antique', u'right', u'old', u'ole', u'deal', u'people', u'successfully', u'crown', u'maxim', u'dead', u'scrimmage', u'intellect', u'humor', u'for', u'creative', u'beating', u'espionage', u'sherman', u'substitutes', u'marketing', u'of', u'conformation', u'properties', u'chapter', u'kohnstamm', u'newspapers', u'dinner', u'dilemma', u'froze', u'intellectual', u'balances', u'down', u'magazines', u'crowd', u'elizabethans', u'flying', u'legislation', u'fight', u'virgin', u'conservation', u\"boy's\", u'way', u'call', u'was', u'head', u'secondhand', u'form', u'offer', u'differences', u'denoting', u'somehow', u'talents', u'hear', u'protestant', u'fairchild', u'attached', u'conformists', u'whiskey', u'lays', u'unusually', u'promises', u'proved', u'suspicions', u\"''\", u'crumbly', u'trip', u'upheaval', u'no', u'glacier', u'stake', u'when', u'proceedings', u'actor', u'signal-to-noise', u'flood', u'role', u'test', u'smell', u'picture', u'handicap', u'felt', u'uniformly', u'diet', u'transported', u'spades', u'longer', u'corresponding', u'vigorously', u'time', u'songs', u'concept', u'impression', u'souls', u'manager', u'depend', u'father', u'charge', u'soloists', u'marks', u'suffered', u'abundance', u'protective', u'advantage', u'could', u'sponsors', u'choice', u'advised', u'cook', u'minute', u'did', u'item', u'team', u'guy', u'round', u'shave', u'spiritual', u'revolution', u'prediction', u'sign', u'eating', u'bales', u'substitute', u'curse', u'spire', u'current', u'goes', u'falling', u'appeal', u'troubled', u'agreement', u'water', u'address', u'alone', u'along', u'teacher', u'change', u'boy', u'weigand', u'accomplished', u'usually', u'hospitality', u'smokies', u'love', u'merely', u'operational', u'working', u'positive', u'angry', u'france', u'territories', u'memory', u'prep', u'today', u'``', u'conductor', u'faith', u'virtue', u'effort', u'organizations', u'pageant', u'car', u'reportage', u'believes', u'regions', u'values', u'can', u'making', u'fulton', u'imitation', u'meant', u'figure', u\"didn't\", u'sample', u'heard', u'clothing', u'productive', u'lucks', u'1', u'economy', u'map', u'staircase', u'may', u'filmstrips', u'produce', u'poignancy', u'representations', u'guys', u'shockingly', u'man', u'stress', u'natural', u'varieties', u'so', u'increase', u'gesture', u'nearly', u'serving', u'furnished', u'indeed', u'mediaevalist', u'years', u'professors', u'shake', u'statements', u'cold', u'still', u'tendency', u'texan', u'pripet', u\"he's\", u'defenseless', u'inferior', u'offers', u'policy', u'endless', u'happened', u'views', u'nation', u'records', u'not', u'now', u'maintaining', u'name', u'residentially', u'possibilities', u'entirely', u'turtle', u'fires', u'begun', u'sexually', u'girl', u'morning', u'krutch', u'intercourse', u'factors', u'rounding', u'tensions', u'looking', u'contained', u'correct', u'shows', u'virtues', u'earlier', u'quite', u'recordings', u'california', u'marina', u'pompons', u'care', u'training', u'workmanship', u'domes', u'british', u'thing', u'place', u'view', u'think', u'first', u'initial', u'revenue', u'sock', u'yourself', u'specifically', u'directly', u'carry', u'array', u'drove', u'george', u'affords', u'size', u'city', u'little', u'necessarily', u'bite', u'anyone', u'plains', u'white', u'structures', u'friend', u'gives', u'nutrition', u'mostly', u'that', u'thieving', u'released', u'herds', u'copy', u'undoubtedly', u'than', u'connections', u'15', u'diplomatic', u'victorian', u'future', u'were', u'russia', u'and', u'tasted', u'remained', u'occupies', u'ani', u'say', u'comedian', u'spectacle', u'breakfast', u'any', u'aside', u'emphasis', u'squad', u'take', u'performance', u'bets', u'sure', u'pain', u'normal', u'knew', u'paid', u'especially', u'considered', u'later', u'filly', u'ways', u'professional', u'dispersed', u'detective', u'show', u'cheap', u'merit', u'gulf', u'genius', u'race-driver', u'proportion', u'crime', u'sloe', u'only', u'going', u'black', u'watching', u'get', u'jazz', u'truly', u\"world's\", u'prime', u'artist', u'dramas', u'summary', u'miles', u'where', u'concert', u'publishers', u'physically', u'seat', u'all-american', u'college', u'eliminating', u'seal', u'concern', u'federal', u\"father's\", u'enough', u'churchgoing', u'between', u'enjoys', u'jobs', u'persians', u\"pip's\", u'singers', u'article', u'cities', u'come', u'concentrated', u'reaction', u'congregations', u'implementing', u'successes', u'many', u'quiet', u'disappointment', u'expression', u\"can't\", u'among', u'color', u'after-effects', u'period', u',', u'exploit', u'learning', u'mark', u'dangers', u'direction', u'offered', u'proven', u'champions', u'those', u'pilot', u'sound', u'turtles', u'these', u'consulted', u'editorial', u'newspaper', u'situation', u'slapping', u'bunters', u'all-round', u'soil', u'strait', u'coffee', u'regularity', u'check', u'deference', u'assigns', u'foundling', u'running', u'favorites', u'theater', u'perennial', u'charges', u'without', u'solve', u'relief', u'justify', u'being', u'money', u'positions', u'speed', u'rose', u'instrument', u'verse', u'bank', u'agoeng', u'real', u\"katharine's\", u'read', u'specimen', u'early', u'world', u'accepted', u'lady', u'fortune', u'stranger', u'pole', u'audience', u'superiority', u'either', u'always', u'endorsed', u'eighteenth-century', u'matching', u'composers', u'slice', u'stops', u'power', u'each', u'throw', u'manufacturer', u'on', u'stone', u'ol', u'central', u'package', u'paints', u'violence', u'sustain', u'side', u'practical', u'preoccupies', u'stand', u'pockets', u'act', u'johnston', u'luck', u'or', u'lands', u'instruments', u'determine', u'parties', u'financial', u'your', u'technically', u'prepare', u'area', u'distinctions', u'there', u'hazards', u'meals', u'start', u'catholics', u'complete', u'acre', u'pitcher', u\"john'll\", u'promote', u'with', u'buying', u\"they're\", u'hire', u'romantic', u'circulation', u\"wasn't\", u'wrings', u'grass', u'gone', u'af', u'taste', u'certain', u'am', u'an', u'as', u'at', u'formulation', u'politics', u'film', u'again', u'rubbed', u\"hangman's\", u'field', u'you', u'poor', u'symbol', u'teeth', u'2:01.1', u'gifts', u'important', u'coverage', u'wife', u'congregationalists', u'having', u'resolution', u'all', u'caused', u'dollar', u'religious', u'children', u'two-record', u'polarity', u\"miller's\", u'thursday', u'former', u'to', u'program', u'smile', u'activities', u'woman', u'song', u'very', u'horror', u'fat', u'balancing', u'got', u'awful', u'difference', u'condition', u'sentimental', u'--', u'small', u'ten', u'tee', u'winds', u'design', u'lawyer', u'healer', u'investment', u'creatures', u'conscience', u'what', u'witnessed', u'version', u'attitudes', u'nurse', u'method', u'full', u'policies', u'puddles', u'trunk', u'strong', u'tortoise', u'losses', u'soldier', u'amount', u'social', u'action', u\"john's\", u'disposition', u'family', u'requiring', u'philco', u'takes', u'contains', u'two', u'pitching', u'achieving', u'minor', u'more', u'flame-throwers', u'door', u'knows', u'company', u'excuse', u\"ain't\", u'varying', u'foundations', u'weathering', u\"orchestra's\", u'debates', u'hour', u'precociously', u'ex-jazz', u'share', u'conceding', u'states', u'minimum', u'numbers', u'sense', u'robinson', u'!', u'information', u'needs', u'court', u'a.b.', u'rather', u'comfort', u'liaison', u'1692', u'detonation', u'blood', u'waves', u'response', u'a', u\"that'd\", u')', u'dread', u'pleasure', u'playing', u\"that's\", u'help', u'scientist', u'through', u'pre-world-war-', u'avenue', u'its', u'greatly', u'actually', u'late', u'lyricists', u'isaac', u'might', u'good', u'tumbler', u'food', u'males', u'socially', u'walls', u'sweeping', u'association', u'occasions', u'truths', u'blest', u'found', u\"isn't\", u'harm', u'mental', u'weight', u'idea', u'expect', u'doses', u'beyond', u'event', u'really', u'since', u'research', u'reverend', u'health', u'print', u'highway', u'friday', u'difficulty', u'reason', u'members', u'temperature', u'definition', u'terrible', u'american', u'prejudice', u'pilots', u'feed', u'feel', u'churning', u'number', u'feet', u'sailor', u'done', u'wages', u'jew', u'judgment', u'entreated', u'least', u'assumption', u'store', u'forests', u'optical', u'boredom', u'believe', u'grace', u'discrimination', u'kind', u'legs', u'recording', u'architect', u'supposed', u'outdoors', u'toward', u'declare', u'strengthen', u'fieldstone', u'defect', u'concentration', u'nights', u'dancing', u'imitations', u'also', u'penman', u'pianist', u'play', u'experienced', u'conspicuously', u'most', u'experiences', u'71', u'position', u'nothing', u'extremely', u'sometimes', u'cover', u'part', u'sinner', u'blend', u'particularly', u'ballplayer', u'defended', u'velvety', u'heavens', u'fine', u'find', u'impact', u'northern', u'pretty', u'circle', u'ten-minute', u'his', u'hit', u'famous', u'reporters', u'during', u'him', u'alexander', u'roles', u'generally', u'palatability', u'common', u'activity', u'set', u'art', u'intelligence', u'culture', u'see', u'individual', u'bare', u'are', u'sea', u'spirits', u'visiting', u'pictures', u\"kid's\", u'case', u'marriage', u\"other's\", u'numerous', u'risks', u'responses', u'attention', u'outfit', u'incident', u'bullhide', u'both', u'prospects', u'cubes', u'restaurant', u'barely', u'foreign', u'sensitive', u'forgotten', u'tarred', u'supply', u'reasons', u'sweet', u'manners', u'whatever', u'danchin', u'dancer', u'simply', u'church', u'slots', u'throughout', u'devil', u'satire', u'damn', u'far', u'meeting', u'champion', u'gay', u\"what's\", u'fund', u'races', u'understand', u'demand', u'conception', u'look', u'solid', u'straight', u'bill', u'tolerate', u'sesame', u'pace', u'while', u'behavior', u'error', u'fun', u'pertinence', u'hoping', u'danes', u'rulers', u'awakening', u'itself', u'aspects', u'sharecrop', u'hocking', u'development', u'literature', u'flurry', u'levels', u'moving', u'cleaning', u'task', u'raimu', u'spent', u'analysis', u'obviously', u'morrow', u'person', u'cheer', u'edge', u'parametric', u\"arlene's\", u'comprise', u'distances', u'judgments', u'shape', u'atomic', u'questions', u'using', u'alignment', u'majority', u'source', u'administrative', u'womanly', u'big', u'squashing', u'matters', u'game', u'biz', u'integer', u'bit', u'helpful', u'stylist', u'glove', u'communications', u'arthur', u'individuals', u'indication', u'popular', u'parade', u'absolutely', u'pals', u'some', u'back', u'bach', u'unpatronizing', u'economic', u'mighty', u'examples', u'fanfare', u'denominations', u'stables', u'tools', u'decision', u'integration', u'providence', u'civilization', u'be', u'run', u'odor', u'lakes', u'step', u'by', u'integers', u'housekeeping', u'anything', u'drama', u'range', u'therapeutic', u'seeing', u'into', u'within', u'neutrality', u\"penny's\", u'oaks', u'lesson', u'rhu-beb-ni-ice', u'bargaining', u'long', u'four-jet', u'impresser', u':', u'himself', u'frost', u'succumbed', u'boys', u'line', u'reek', u'characteristic', u'sounds', u'up', u'us', u'planet', u'coincides', u'influence', u'single', u'warning', u'treaty', u'peace', u'points', u'actors', u'feare', u'mock', u'nice', u'understatement', u'lens', u'fertilizer', u'desert', u'interfaith', u'land', u'fighter', u'age', u'required', u'cooked', u'depth', u'walker', u'fresh', u'requires', u'sounded', u'results', u'go', u'contributions', u'issues', u'seemed', u'suave', u'send', u'whisky', u'waiters', u'wished', u'garden', u'wave', u'wishes', u'trough', u'telling', u'drinking', u'knight', u'karipo', u'try', u'race', u'epstein', u'mysticisms', u'crop', u'rivers', u'business', u'harvests', u\"man's\", u'access', u'flowing', u';', u'ammunition', u'exercise', u'body', u'bachelor', u'let', u'lubricant', u'great', u'engage', u'talent', u'technical', u'larger', u'defeat', u'makes', u'danger', u'private', u'hardy', u'names', u'singing', u'role-experimentation', u'standing', u'use', u'from', u'few', u'assuring', u'oily', u'sort', u'becomes', u'occurred', u'charming', u'train', u'baby', u\"'ceptin'\", u'hints', u'customer', u'sanctions', u'this', u'contagion', u'island', u'proof', u'control', u'earnestness', u'favorite', u'rode', u'high', u'pulling', u'something', u'publicity', u'skirt', u'educational', u'fortunate', u'delay', u'sentiments', u'animal', u'comedy', u'establishment', u'hafta', u'stock', u'tension', u'buildings', u'blocks', u'farm', u'tact', u'collection', u'polarities', u'light', u'element', u'martini', u'furnish', u'brahms', u'lullwater', u'produced', u'including', u'looks', u\"player's\", u'industries', u'superior', u'brood', u'decay', u'chosen', u'communication', u'orange', u'designs', u'greater', u'dam', u'practice', u'mention', u'hands', u\"heat's\", u'day', u'mastery', u'sufficiently', u'truth', u'promise', u'bills', u'lilac', u'doing', u'administrator', u'related', u'society', u'books', u'measure', u'our', u'salesman', u'out', u\"'\", u'performs', u'frankness', u'red', u\"one's\", u'wines', u'frank', u'undo', u'completely', u'steering', u'her', u'route', u'1923-27', u'times', u'length', u'south', u'strategic', u'such', u'precisely', u'quality', u\"you're\", u'bears', u'traveller', u'relations', u'their', u'perfectly', u'beard', u'explanation', u'exactly', u'reflecting', u'ben', u'riches', u'listeners', u'reproduction', u'embody', u'dumb', u'providing', u'exhibit', u'westward', u'tonight', u'sincerity', u'have', u'need', u'apparently', u'acceleration', u'instrumentation', u'bastard', u'incentive', u'clodhoppers', u'which', u'cozen', u'who', u'intentions', u'eight', u'cracking', u'intrinsic', u'stereo', u'thanksgiving', u'looked', u\"you'll\", u'occasion', u'painting', u'son', u'affair', u'atmosphere', u'chances', u'chapters', u'planning', u'democrats', u'debate', u'highest', u'hops', u'(', u'controls', u'should', u'employee', u'he', u'hope', u'photographs', u'means', u'ones', u'fallout', u'probably', u'spur', u'areas', u'organ', u'conditions', u'calling', u'she', u'fixed', u'widow', u'guessing', u'acquired', u'national', u'edition', u'usefulness', u'record', u'favor', u'state', u'difficulties', u'identification', u'injustice', u'progress', u'variability', u'importance', u'sorrow', u'strides', u'job', u'joe', u'approval', u'sympathizing', u'joke', u'minds', u'admit', u'figures', u'otherwise', u'walk', u'cousin', u'table', u'trays', u\"it's\", u'poet', u\"it'd\", u'faced', u'senses', u'controlled', u'received', u'essentially', u'assert', u'present', u'homes', u'novel', u'britain', u'value', u'will', u'discoveries', u'almost', u'thus', u'site', u'surface', u'helped', u'swinging', u'aggressiveness', u'perhaps', u'vintage', u'balloon', u'member', u'parts', u'speaker', u'dickens', u'ball', u'drink', u'upon', u'effect', u'tsunami', u\"era's\", u'frequently', u'colts', u'collar', u'identity', u'keep', u'scarcely', u'off', u'reflection', u'i', u'well', u'thought', u'english', u'undertaken', u'less', u'increasingly', u'dose', u'skill', u'smith', u'lake', u'recommending', u'realize', u'simple', u'other', u'increased', u'government', u'know', u'burden', u'czarina', u'loss', u'necessary', u'like', u'success', u'lose', u'become', u'works', u'backlog', u'italian', u'because', u'classical', u'authority', u'choosing', u'home', u'hissing', u'avoid', u'lean', u'leap', u'does', u'?', u'biology', u'paper', u'pressure', u'benefit', u'although', u'slum', u'vs.', u'about', u'getting', u'column', u'equally', u'own', u'letters', u'pussy', u'weather', u'h.', u'artists', u'disapproval', u'notices', u'exposure', u'buy', u'north', u'but', u'landscaping', u'plague', u'courts', u'ear', u'partially', u'baptist', u'made', u'places', u'whether', u'wish', u'mists', u'sufferer', u'excitement', u'placed', u'turnings', u'burdened', u'chouise', u'demonstrate', u'problem', u'piece', u'bearing', u'year', u'pie', u'abaringe', u'periods', u'education', u'mutual', u'variety', u'corporation', u'guitarist', u'oak', u'detail', u'book', u'repute', u'chance', u'gatsby', u'friends', u'cliff', u'starting', u'baker', u'space', u'portion', u'uneducated']\n",
      "[{'good': Counter({u'a': 254, u',': 68, u'the': 56, u'was': 55, u'is': 54, u'for': 54, u'and': 53, u'of': 50, u'.': 47, u'in': 41, u'as': 38, u'to': 37, u'be': 32, u'``': 27, u'deal': 27, u'with': 20, u'one': 17, u'will': 16, u'not': 15, u'no': 14, u'it': 13, u'are': 13, u'been': 13, u'had': 13, u'very': 12, u'were': 12, u'have': 12, u'some': 11, u'or': 11, u\"''\": 11, u'time': 11, u'man': 10, u'that': 10, u'many': 10, u'luck': 10, u'reason': 9, u'thing': 9, u'his': 9, u'him': 9, u'so': 8, u'pretty': 8, u'job': 8, u'look': 8, u'he': 8, u'at': 8, u'health': 7, u\"it's\": 7, u'on': 7, u'from': 6, u'idea': 6, u'too': 6, u'which': 6, u'enough': 6, u'make': 6, u'--': 6, u'old': 6, u'by': 6, u'made': 6, u'results': 5, u'what': 5, u'men': 5, u'looks': 5, u'reasonably': 5, u'year': 5, u'another': 5, u'friend': 5, u'nothing': 5, u'only': 5, u'do': 5, u'other': 5, u'all': 4, u'condition': 4, u'two': 4, u'they': 4, u'feel': 4, u'than': 4, u'any': 4, u'intentions': 4, u'looked': 4, u'ones': 4, u'news': 4, u'feeling': 4, u'teachers': 4, u'just': 4, u'being': 4, u'works': 4, u'would': 4, u'into': 4, u'her': 4, u'way': 4, u'taste': 4, u'felt': 4, u'friends': 4, u'seemed': 3, u'me': 3, u'did': 3, u';': 3, u'boy': 3, u'use': 3, u'customer': 3, u'work': 3, u'example': 3, u'still': 3, u'food': 3, u'day': 3, u'sufficiently': 3, u'found': 3, u'out': 3, u'friday': 3, u'could': 3, u'first': 3, u'such': 3, u'little': 3, u'service': 3, u'their': 3, u'also': 3, u'who': 3, u'get': 3, u'said': 3, u'between': 3, u'both': 3, u'fairly': 3, u'i': 3, u'greatest': 3, u'has': 3, u'like': 3, u'fortune': 3, u'become': 3, u'because': 3, u'does': 3, u'start': 3, u'form': 3, u'but': 3, u'up': 3, u'an': 3, u'looking': 2, u'lord': 2, u'difference': 2, u'literature': 2, u'approximation': 2, u'conscience': 2, u'body': 2, u'full': 2, u\"i'm\": 2, u'family': 2, u'working': 2, u'faith': 2, u'company': 2, u'this': 2, u'can': 2, u'my': 2, u'something': 2, u'sense': 2, u'how': 2, u'may': 2, u'after': 2, u'froze': 2, u'guys': 2, u'furnish': 2, u'years': 2, u'own': 2, u'its': 2, u\"he's\": 2, u'grade': 2, u'them': 2, u'practice': 2, u'now': 2, u'always': 2, u'harm': 2, u'society': 2, u'girl': 2, u'salesman': 2, u'fellowship': 2, u'got': 2, u'days': 2, u'times': 2, u'there': 2, u'nutrition': 2, u'future': 2, u'seen': 2, u'cause': 2, u'order': 2, u\"that's\": 2, u'impression': 2, u'show': 2, u'part': 2, u'hope': 2, u'?': 2, u'stead': 2, u'common': 2, u'seat': 2, u'we': 2, u'never': 2, u'nature': 2, u'she': 2, u'reasons': 2, u'whatever': 2, u'much': 2, u'behavior': 2, u'several': 2, u'effect': 2, u'without': 2, u'english': 2, u'musical': 2, u'money': 2, u'judge': 2, u'bit': 2, u'mighty': 2, u'examples': 2, u'humor': 2, u'night': 2, u'about': 2, u'getting': 2, u'weather': 2, u'legislation': 2, u':': 2, u'catholics': 2, u'measure': 2, u'you': 2, u'really': 2, u'sounded': 2, u'stock': 1, u'concept': 1, u'dollar': 1, u'facilities': 1, u'manager': 1, u'go': 1, u'appetite': 1, u'depend': 1, u'father': 1, u'thursday': 1, u'marks': 1, u'abundance': 1, u'garden': 1, u'advantage': 1, u'song': 1, u'sponsors': 1, u'fat': 1, u'choice': 1, u'wishes': 1, u'balancing': 1, u'cook': 1, u'sentimental': 1, u'knight': 1, u'try': 1, u'epstein': 1, u'team': 1, u'guy': 1, u\"one's\": 1, u'graces': 1, u'clothes': 1, u'force': 1, u'specially': 1, u'ten': 1, u'mysticisms': 1, u'estimates': 1, u'japanese': 1, u'second': 1, u'eating': 1, u'investment': 1, u'sesame': 1, u'christ': 1, u'meals': 1, u'orchestra': 1, u'flowing': 1, u'new': 1, u'non-violence': 1, u'policies': 1, u'agreement': 1, u'let': 1, u'precociously': 1, u'alone': 1, u'strong': 1, u'teacher': 1, u'obtained': 1, u'great': 1, u'voice': 1, u'larger': 1, u'weigand': 1, u'accomplished': 1, u'defeat': 1, u'real': 1, u'hospitality': 1, u\"john's\": 1, u'infield': 1, u'private': 1, u'brought': 1, u'hardy': 1, u'send': 1, u'total': 1, u'yourself': 1, u'conformists': 1, u'throw': 1, u'few': 1, u'assuring': 1, u'music': 1, u'pitching': 1, u'memory': 1, u'tell': 1, u'minor': 1, u'door': 1, u'knows': 1, u'initial': 1, u'under': 1, u'excuse': 1, u'hold': 1, u'housekeeping': 1, u'word': 1, u'believes': 1, u'mechanic': 1, u'making': 1, u'president': 1, u\"didn't\": 1, u'conceding': 1, u'sample': 1, u'minimum': 1, u'pulling': 1, u'want': 1, u'clothing': 1, u'productive': 1, u'end': 1, u'provide': 1, u'lucks': 1, u'debauchery': 1, u'writers': 1, u'animal': 1, u'comedy': 1, u'economy': 1, u'map': 1, u'ball': 1, u'representations': 1, u'pilots': 1, u'rather': 1, u'playing': 1, u'wine': 1, u'furnished': 1, u'produced': 1, u'awful': 1, u'through': 1, u'before': 1, u'brood': 1, u'better': 1, u'distinguishing': 1, u'minute': 1, u'might': 1, u'greater': 1, u'combination': 1, u'dam': 1, u'records': 1, u'walls': 1, u'nurse': 1, u'glazed': 1, u'maintaining': 1, u'meat': 1, u'each': 1, u'bills': 1, u'meal': 1, u'lasting': 1, u'turtle': 1, u'directly': 1, u'doing': 1, u'related': 1, u'used': 1, u'begun': 1, u'beyond': 1, u'navigator': 1, u'intercourse': 1, u'factors': 1, u'god': 1, u'increase': 1, u'flushing': 1, u'print': 1, u'frankness': 1, u'remoteness': 1, u'drink': 1, u'wines': 1, u'negociant': 1, u'disposition': 1, u'mothers-in-law': 1, u'care': 1, u'definition': 1, u'workmanship': 1, u'created': 1, u'british': 1, u'liaison': 1, u'place': 1, u'view': 1, u'number': 1, u'ring': 1, u'clarity': 1, u'awfully': 1, u\"you're\": 1, u'necessarily': 1, u'jew': 1, u'virgin': 1, u'judgment': 1, u'bite': 1, u'entreated': 1, u'least': 1, u'name': 1, u'percentage': 1, u'perfectly': 1, u'exactly': 1, u'took': 1, u'optical': 1, u'probably': 1, u'believe': 1, u'kind': 1, u'john': 1, u'ben': 1, u'tree': 1, u'matter': 1, u'listeners': 1, u'marriage': 1, u'supposed': 1, u'providing': 1, u'usefulness': 1, u'tasted': 1, u'strengthen': 1, u'occupies': 1, u'ani': 1, u'tonight': 1, u'sportsmen': 1, u'himself': 1, u'comedian': 1, u'sincerity': 1, u'breakfast': 1, u'apparently': 1, u'secondhand': 1, u'instrumentation': 1, u'wages': 1, u'aside': 1, u'squad': 1, u'take': 1, u'performance': 1, u'cozen': 1, u'bets': 1, u'sure': 1, u'paid': 1, u'eight': 1, u'cracking': 1, u'extremely': 1, u'stereo': 1, u'doo': 1, u'landscaping': 1, u'considered': 1, u'professor': 1, u'filly': 1, u'points': 1, u'blend': 1, u'detective': 1, u'particularly': 1, u'ballplayer': 1, u'planning': 1, u'radio': 1, u'telling': 1, u'heavens': 1, u'find': 1, u'promote': 1, u'judgments': 1, u'hops': 1, u'(': 1, u'should': 1, u'sloe': 1, u'going': 1, u'employee': 1, u\"arlene's\": 1, u'meant': 1, u'yff': 1, u'hit': 1, u'churchgoing': 1, u'jazz': 1, u'hire': 1, u'despite': 1, u'report': 1, u'15': 1, u'palatability': 1, u'morning': 1, u'miles': 1, u'wish': 1, u'widow': 1, u'physically': 1, u'intelligence': 1, u'connections': 1, u'individual': 1, u'college': 1, u'eliminating': 1, u'record': 1, u'pictures': 1, u'mayor': 1, u\"father's\": 1, u\"there's\": 1, u'cooperation': 1, u'architect': 1, u'enjoys': 1, u\"other's\": 1, u'chouise': 1, u'jobs': 1, u'outfit': 1, u'implementing': 1, u'prospects': 1, u'cow': 1, u'restaurant': 1, u'country': 1, u'barely': 1, u'linguist-anthropologist': 1, u'figures': 1, u'distinction': 1, u'expression': 1, u'otherwise': 1, u'character': 1, u'address': 1, u'walk': 1, u'dancer': 1, u'church': 1, u'springs': 1, u'slots': 1, u'most': 1, u'binge': 1, u'damn': 1, u'acre': 1, u'warmth': 1, u'life': 1, u'modern': 1, u'gay': 1, u'received': 1, u\"what's\": 1, u'worker': 1, u'flurry': 1, u'engage': 1, u'those': 1, u'sound': 1, u'homes': 1, u'novel': 1, u'solid': 1, u'operational': 1, u'straight': 1, u'bill': 1, u'say': 1, u'while': 1, u'slapping': 1, u'balance': 1, u'shortened': 1, u'bunters': 1, u'hoping': 1, u'almost': 1, u'soil': 1, u'site': 1, u'sharecrop': 1, u'things': 1, u'parts': 1, u'tumbler': 1, u'hocking': 1, u'males': 1, u'catholic': 1, u'social': 1, u'running': 1, u'levels': 1, u'purpose': 1, u'colts': 1, u'scarcely': 1, u'off': 1, u'perennial': 1, u'thy': 1, u'well': 1, u'spent': 1, u'morrow': 1, u'person': 1, u'cheer': 1, u'dose': 1, u'shape': 1, u'world': 1, u'antiseptic': 1, u'lake': 1, u'recommending': 1, u'verse': 1, u'cognac': 1, u'decorator': 1, u'easy': 1, u'sweet': 1, u'crops': 1, u'city': 1, u'agoeng': 1, u'maye': 1, u'womanly': 1, u'government': 1, u'read': 1, u'specimen': 1, u'doses': 1, u'biz': 1, u'know': 1, u'penman': 1, u'cape': 1, u'lady': 1, u'helpful': 1, u'audience': 1, u'indication': 1, u'lose': 1, u'endorsed': 1, u'baptist': 1, u'antique': 1, u'ole': 1, u'people': 1, u'back': 1, u'authority': 1, u'standing': 1, u'scrimmage': 1, u'choosing': 1, u'slice': 1, u'decision': 1, u'stops': 1, u'offices': 1, u'trunk': 1, u'biology': 1, u'run': 1, u'marketing': 1, u'conformation': 1, u'although': 1, u'drama': 1, u'dinner': 1, u'preoccupies': 1, u'pockets': 1, u'mixture': 1, u'lands': 1, u'letter': 1, u'instruments': 1, u'son': 1, u'magazines': 1, u'lesson': 1, u'mark': 1, u'your': 1, u'classical': 1, u'flying': 1, u\"kid's\": 1, u'long': 1, u'fight': 1, u'conservation': 1, u'impresser': 1, u'notices': 1, u'buy': 1, u'passport': 1, u'offer': 1, u'crown': 1, u'frost': 1, u'boys': 1, u'highest': 1, u'buying': 1, u\"they're\": 1, u'protestant': 1, u'unpatronizing': 1, u\"wasn't\": 1, u'characteristic': 1, u'us': 1, u'prep': 1, u'placed': 1, u'books': 1, u'today': 1, u'unusually': 1, u'grass': 1, u'proved': 1, u'am': 1, u'pie': 1, u'abaringe': 1, u'crumbly': 1, u'formulation': 1, u'politics': 1, u'education': 1, u'mutual': 1, u'corporation': 1, u'peace': 1, u'generally': 1, u'signal-to-noise': 1, u'field': 1, u'role': 1, u'repute': 1, u'smell': 1, u'mock': 1, u'symbol': 1, u'uniformly': 1, u'diet': 1, u'chance': 1, u'coverage': 1, u'fertilizer': 1, u'desert': 1, u'interfaith': 1, u'land': 1, u'fighter': 1, u\"'\": 1, u'wife': 1, u'having': 1, u'cooked': 1, u'portion': 1, u'songs': 1})}, {'nice': Counter({u'a': 27, u',': 14, u'was': 10, u'to': 6, u'``': 6, u'with': 5, u'and': 5, u'very': 4, u'some': 4, u'it': 4, u\"''\": 4, u'as': 4, u'people': 3, u'be': 3, u'about': 3, u'of': 3, u\"it's\": 3, u'have': 3, u'such': 3, u'so': 3, u'all': 2, u'not': 2, u\"he's\": 2, u'awfully': 2, u'day': 2, u'--': 2, u'guy': 2, u'for': 2, u'place': 2, u'little': 2, u'this': 2, u'something': 2, u'is': 2, u'cold': 1, u'two-record': 1, u'chosen': 1, u'smile': 1, u'has': 1, u'ten-minute': 1, u'real': 1, u'then': 1, u'his': 1, u\"katharine's\": 1, u'awful': 1, u'lady': 1, u'these': 1, u'man': 1, u'fixed': 1, u'where': 1, u'round': 1, u\"isn't\": 1, u'widow': 1, u'girl': 1, u'since': 1, u'lean': 1, u'looking': 1, u'run': 1, u'met': 1, u'bachelor': 1, u'on': 1, u'anything': 1, u'package': 1, u'violence': 1, u'quiet': 1, u'airport': 1, u'or': 1, u'pussy': 1, u'balances': 1, u'down': 1, u'rhu-beb-ni-ice': 1, u'singing': 1, u'her': 1, u'.': 1, u'way': 1, u'legs': 1, u'sort': 1, u'mostly': 1, u'received': 1, u'released': 1, u'baby': 1, u'ear': 1, u'romantic': 1, u'restaurant': 1, u'up': 1, u'planet': 1, u'were': 1, u'country': 1, u'an': 1, u'imitation': 1, u'in': 1, u'perhaps': 1, u'when': 1, u'understatement': 1, u'kid': 1, u'thought': 1, u'fresh': 1, u'the': 1})}, {'excellent': Counter({u'an': 23, u',': 11, u'and': 6, u'is': 6, u'for': 5, u'in': 5, u'are': 4, u'was': 4, u'example': 4, u'to': 3, u'his': 3, u'some': 2, u'spirits': 2, u'article': 2, u'feet': 2, u'.': 2, u'with': 2, u'have': 2, u'show': 1, u'chances': 1, u'both': 1, u'shape': 1, u'paper': 1, u'skill': 1, u'still': 1, u'whose': 1, u\"miller's\": 1, u'had': 1, u'soloists': 1, u'photographs': 1, u'designs': 1, u'advantage': 1, u'government': 1, u'very': 1, u'practice': 1, u'famous': 1, u'were': 1, u'fallout': 1, u'reporters': 1, u'schools': 1, u'condition': 1, u'psychological': 1, u'--': 1, u'conditions': 1, u'summary': 1, u'blest': 1, u'bone': 1, u'italian': 1, u'concert': 1, u'lilac': 1, u'maxim': 1, u'crop': 1, u'design': 1, u'matter': 1, u'rounding': 1, u'integration': 1, u'health': 1, u'performs': 1, u'ammunition': 1, u'numerous': 1, u'be': 1, u\"doctor's\": 1, u'agreement': 1, u'job': 1, u'chapter': 1, u'on': 1, u'restaurant': 1, u'paints': 1, u'foreign': 1, u'thing': 1, u'place': 1, u'makes': 1, u'into': 1, u'one': 1, u'h.': 1, u'affords': 1, u'her': 1, u'instrument': 1, u'there': 1, u'two': 1, u'music': 1, u'too': 1, u'way': 1, u'legs': 1, u'gives': 1, u'that': 1, u'offered': 1, u'conspicuously': 1, u'but': 1, u'part': 1, u'hear': 1, u'weathering': 1, u'pilot': 1, u'fairchild': 1, u'this': 1, u'pace': 1, u'editorial': 1, u'of': 1, u'lubricant': 1, u'providing': 1, u'all-round': 1, u'it': 1, u'as': 1, u'stint': 1, u'containing': 1, u'dancing': 1, u')': 1, u'vintage': 1, u'trip': 1, u'how': 1, u'book': 1, u'which': 1, u'pianist': 1, u'play': 1, u'uniformly': 1, u'collection': 1, u'filmstrips': 1, u'71': 1, u'potentials': 1, u'such': 1, u'network': 1, u'demand': 1, u'possessed': 1, u'depth': 1, u'reputation': 1, u'the': 1})}, {'positive': Counter({u'a': 17, u'the': 16, u'of': 12, u',': 7, u'are': 6, u'and': 5, u'or': 4, u'is': 4, u'kohnstamm': 3, u'be': 3, u'.': 3, u'was': 3, u'more': 3, u'``': 3, u'no': 3, u'that': 3, u'give': 3, u'results': 2, u'not': 2, u'integer': 2, u'set': 2, u'some': 2, u'state': 2, u'by': 2, u\"i'm\": 2, u'one': 2, u'means': 2, u'but': 2, u\"''\": 2, u'in': 2, u'which': 2, u'you': 2, u'reaction': 2, u'lens': 2, u'on': 2, u'affair': 1, u'feedback': 1, u'still': 1, u'religious': 1, u'its': 1, u'to': 1, u'charge': 1, u'program': 1, u'gave': 1, u'his': 1, u'views': 1, u'very': 1, u'feelings': 1, u'--': 1, u'benefit': 1, u'side': 1, u'doing': 1, u'what': 1, u'pressures': 1, u'we': 1, u'step': 1, u'approval': 1, u'both': 1, u'about': 1, u'congregations': 1, u'contagion': 1, u'stand': 1, u'act': 1, u'action': 1, u'control': 1, u'features': 1, u'color': 1, u'pole': 1, u'lesson': 1, u'from': 1, u'contains': 1, u'negative': 1, u'least': 1, u'their': 1, u'offers': 1, u'only': 1, u'controlled': 1, u'integers': 1, u'successful': 1, u'differences': 1, u'g.o.p.': 1, u'took': 1, u'varying': 1, u'foundations': 1, u'partially': 1, u'high': 1, u'preconditioned': 1, u'value': 1, u'my': 1, u'proof': 1, u'af': 1, u'almost': 1, u'sense': 1, u'it': 1, u'conditioned': 1, u'something': 1, u'incentive': 1, u'take': 1, u'development': 1, u'assigns': 1, u'normal': 1, u'handicap': 1, u'qualifications': 1, u'most': 1, u'longer': 1, u'i': 1, u'parametric': 1, u'thanksgiving': 1, u'element': 1, u\"you'll\": 1, u'without': 1, u'so': 1, u'greatest': 1, u'position': 1, u'resolution': 1})}, {'fortunate': Counter({u'less': 6, u'was': 5, u',': 4, u'enough': 3, u'in': 2, u'.': 2, u'for': 2, u'they': 2, u'the': 2, u'all': 1, u'proved': 1, u'members': 1, u'is': 1, u'some': 1, u'souls': 1, u'one': 1, u\"''\": 1, u'result': 1, u'have': 1, u'children': 1, u'from': 1, u'been': 1, u'anyone': 1, u'how': 1, u'``': 1, u'that': 1, u'were': 1, u'however': 1, u'czarina': 1, u'with': 1, u'realize': 1, u'he': 1, u'a': 1, u'prepare': 1, u'occasions': 1, u'this': 1, u'of': 1, u'nations': 1, u'these': 1, u'time': 1, u'exceptions': 1})}, {'correct': Counter({u'to': 12, u'the': 10, u'is': 8, u',': 6, u'.': 6, u'in': 6, u'of': 5, u'a': 5, u'as': 4, u'--': 3, u'their': 3, u'was': 3, u'this': 3, u'not': 2, u'are': 2, u'were': 2, u'quite': 2, u'supply': 2, u'can': 2, u'and': 2, u'it': 2, u'order': 2, u'atmosphere': 1, u'interpretation': 1, u'mastery': 1, u'years': 1, u'avenue': 1, u'polarity': 1, u'how': 1, u'(': 1, u'suave': 1, u'proportion': 1, u'attitudes': 1, u'administrative': 1, u'now': 1, u'positions': 1, u'method': 1, u'try': 1, u'item': 1, u'observance': 1, u'absolutely': 1, u'some': 1, u'prediction': 1, u'observed': 1, u'lawyer': 1, u'matching': 1, u'speed': 1, u'research': 1, u'current': 1, u'version': 1, u'identification': 1, u'injustice': 1, u';': 1, u'be': 1, u'pressure': 1, u'study': 1, u'decay': 1, u'times': 1, u'length': 1, u'or': 1, u'statements': 1, u'neutrality': 1, u'danchin': 1, u'done': 1, u'determine': 1, u'precisely': 1, u'her': 1, u'he': 1, u'will': 1, u'reproduction': 1, u'krutch': 1, u'proved': 1, u'figure': 1, u\"''\": 1, u'have': 1, u'amount': 1, u'also': 1, u'development': 1, u'most': 1, u'polarities': 1, u'vigorously': 1, u'analysis': 1, u'think': 1, u'without': 1, u'so': 1, u'position': 1})}, {'superior': Counter({u'court': 12, u'in': 8, u'a': 7, u'the': 6, u'to': 4, u'and': 4, u'is': 3, u',': 3, u'.': 3, u'courts': 3, u'with': 3, u'of': 3, u'be': 2, u'virtue': 2, u'new': 2, u'fulton': 2, u'exhibit': 1, u'feed': 1, u'right': 1, u'force': 1, u'just': 1, u'people': 1, u'family': 1, u'marksmanship': 1, u'felt': 1, u\"''\": 1, u'culture': 1, u'consulted': 1, u'properties': 1, u'are': 1, u'another': 1, u'aggressiveness': 1, u'some': 1, u'fine': 1, u\"player's\": 1, u'sexually': 1, u'technically': 1, u'by': 1, u'accepted': 1, u'i': 1, u'stables': 1, u'generally': 1, u'only': 1, u'state': 1, u'civilization': 1, u'upon': 1, u'under': 1, u'performance': 1, u'two': 1, u'several': 1, u'was': 1, u'sort': 1, u'``': 1, u'his': 1, u'champion': 1, u'that': 1, u'may': 1, u'mind': 1, u'who': 1, u'pitcher': 1, u'important': 1, u'california': 1, u'not': 1, u'vastly': 1, u'creatures': 1, u'former': 1, u'definition': 1, u'reflection': 1, u'showed': 1, u'heard': 1, u'--': 1, u'possessed': 1, u'no': 1, u'diplomatic': 1, u'as': 1, u'or': 1, u'person': 1, u'american': 1, u'race': 1, u'enterprise': 1, u'confronted': 1, u'other': 1})}, {'great': Counter({u'the': 173, u'a': 166, u'of': 93, u',': 46, u'deal': 43, u'is': 39, u'in': 32, u'and': 30, u'to': 28, u'with': 23, u'as': 22, u'that': 21, u'not': 18, u'``': 17, u'many': 17, u'was': 17, u'for': 15, u'have': 14, u'so': 13, u'be': 12, u'.': 11, u'britain': 11, u'this': 10, u'are': 10, u'too': 9, u'his': 9, u'at': 9, u'--': 8, u'had': 8, u'expectations': 8, u'by': 8, u'very': 7, u'from': 7, u'man': 7, u'were': 7, u'no': 7, u'but': 7, u'made': 7, u'its': 6, u'their': 6, u'been': 6, u'has': 6, u'an': 6, u'our': 5, u'number': 5, u'any': 5, u'importance': 5, u'majority': 5, u'on': 5, u'there': 5, u'two': 4, u'american': 4, u'such': 4, u'personal': 4, u'them': 4, u'need': 4, u'do': 4, u'men': 4, u'among': 4, u'value': 4, u'power': 4, u'found': 4, u'her': 4, u'caused': 3, u'advantage': 3, u'difference': 3, u'rivers': 3, u'change': 3, u'smoky': 3, u'train': 3, u'after': 3, u'stress': 3, u'sufficiently': 3, u'harm': 3, u'cause': 3, u'difficulty': 3, u'length': 3, u'white': 3, u'also': 3, u'which': 3, u'help': 3, u'extent': 3, u'country': 3, u'interest': 3, u'own': 3, u'variety': 3, u'flood': 3, u'age': 3, u'all': 2, u'contributions': 2, u'pride': 2, u'garden': 2, u'surprise': 2, u'second': 2, u'appeal': 2, u'technical': 2, u'danger': 2, u'takes': 2, u'my': 2, u'numbers': 2, u'sense': 2, u'truly': 2, u'through': 2, u'still': 2, u'writing': 2, u'good': 2, u'nation': 2, u'hands': 2, u'him': 2, u'literature': 2, u'event': 2, u'got': 2, u'struggle': 2, u'care': 2, u'first': 2, u'already': 2, u'one': 2, u'done': 2, u'size': 2, u'plains': 2, u'big': 2, u'took': 2, u'nations': 2, u'architect': 2, u'spectacle': 2, u'emphasis': 2, u'without': 2, u'areas': 2, u'detective': 2, u'pope': 2, u'black': 2, u'dramas': 2, u'she': 2, u'national': 2, u'concern': 2, u'project': 2, u'it': 2, u'we': 2, u'three': 2, u'life': 2, u'periods': 2, u'awakening': 2, u'evil': 2, u'obviously': 2, u'edge': 2, u'detail': 2, u'source': 2, u'motivating': 2, u'advances': 2, u'world': 2, u'loss': 2, u'like': 2, u'success': 2, u'some': 2, u'or': 2, u'seeing': 2, u'into': 2, u\"world's\": 2, u'head': 2, u'north': 2, u'up': 2, u'problem': 2, u'am': 2, u\"''\": 2, u'stake': 2, u'other': 2, u'land': 2, u'green': 1, u\"detective's\": 1, u'four': 1, u'go': 1, u'issues': 1, u'cliff': 1, u'contained': 1, u'presents': 1, u'program': 1, u'present': 1, u'under': 1, u'suffered': 1, u'waiters': 1, u'activities': 1, u'protective': 1, u'misunderstanding': 1, u'wave': 1, u'trough': 1, u'drinking': 1, u'states': 1, u'companies': 1, u'karipo': 1, u'round': 1, u'shave': 1, u'spiritual': 1, u'revolution': 1, u'force': 1, u'guidelines': 1, u'healer': 1, u'bales': 1, u'hamper': 1, u'even': 1, u'curse': 1, u'witnessed': 1, u'religious': 1, u'rockies': 1, u'spire': 1, u\"man's\": 1, u'access': 1, u'oaks': 1, u'inducement': 1, u'era': 1, u'never': 1, u'lakes': 1, u'puddles': 1, u'newspaper': 1, u'along': 1, u'teacher': 1, u'talent': 1, u'tortoise': 1, u'losses': 1, u'absorbency': 1, u'soldier': 1, u'social': 1, u'changes': 1, u'criticism': 1, u'love': 1, u'requiring': 1, u'explained': 1, u'psychological': 1, u'moral': 1, u'names': 1, u'use': 1, u'philco': 1, u'would': 1, u'france': 1, u'few': 1, u'achieving': 1, u'flame-throwers': 1, u'conductor': 1, u'thus': 1, u'marks': 1, u'wars': 1, u'aware': 1, u'effort': 1, u'hints': 1, u\"orchestra's\": 1, u'organizations': 1, u'pageant': 1, u'debates': 1, u'work': 1, u'attention': 1, u'regularity': 1, u'groves': 1, u'can': 1, u'island': 1, u'field': 1, u'squirting': 1, u'figure': 1, u'earnestness': 1, u'rode': 1, u'ashikaga': 1, u'domes': 1, u'involve': 1, u'caution': 1, u'present-day': 1, u'robinson': 1, u'!': 1, u'information': 1, u'needs': 1, u'a.b.': 1, u'provide': 1, u'skirt': 1, u'means': 1, u'1': 1, u'1692': 1, u'tension': 1, u'buildings': 1, u'blocks': 1, u'elizabeth': 1, u'staircase': 1, u'may': 1, u'tact': 1, u'educational': 1, u'collection': 1, u'distances': 1, u'waves': 1, u'guys': 1, u'misleads': 1, u'natural': 1, u'varieties': 1, u'light': 1, u'appreciate': 1, u'greek': 1, u'complexity': 1, u'ultimate': 1, u'pleasure': 1, u'democratic': 1, u'make': 1, u'serving': 1, u'brahms': 1, u'merit': 1, u'toward': 1, u'find': 1, u'mediaevalist': 1, u'lullwater': 1, u'professors': 1, u'scientist': 1, u'pre-world-war-': 1, u'dilemma': 1, u'tendency': 1, u'industries': 1, u'texan': 1, u'pripet': 1, u'2:06': 1, u\"south's\": 1, u'orange': 1, u'alexander': 1, u'might': 1, u'overcome': 1, u'weakness': 1, u'safe': 1, u'mention': 1, u'strategic': 1, u'always': 1, u'residentially': 1, u'truths': 1, u'possibilities': 1, u'victory': 1, u'did': 1, u'water': 1, u'financial': 1, u'mental': 1, u'weight': 1, u'series': 1, u'parties': 1, u'fires': 1, u'year': 1, u'out': 1, u'beauty': 1, u'trumpeter': 1, u'god': 1, u'since': 1, u'increase': 1, u'laid': 1, u'sustain': 1, u'engineer': 1, u'highway': 1, u\"one's\": 1, u'earlier': 1, u'recordings': 1, u'advantages': 1, u'marina': 1, u'pompons': 1, u'where': 1, u'route': 1, u'cosmic': 1, u'terrible': 1, u'1923-27': 1, u'times': 1, u'creates': 1, u'prejudice': 1, u'stone': 1, u'south': 1, u'characters': 1, u'revenue': 1, u'churning': 1, u'poignancy': 1, u'another': 1, u'carry': 1, u'array': 1, u'drove': 1, u'whisky': 1, u'service': 1, u'traveller': 1, u'structures': 1, u'store': 1, u'beard': 1, u'urban': 1, u'murder': 1, u'way': 1, u'wisdom': 1, u'reflecting': 1, u'herds': 1, u'sign': 1, u'undoubtedly': 1, u'distance': 1, u'roles': 1, u'riches': 1, u'recording': 1, u'victorian': 1, u'risks': 1, u'historical': 1, u'outdoors': 1, u'embody': 1, u'feeling': 1, u'powers': 1, u'russia': 1, u'raimu': 1, u'remained': 1, u'ran': 1, u'fieldstone': 1, u'westward': 1, u'all-time': 1, u'acceleration': 1, u'nights': 1, u'dancing': 1, u'take': 1, u'performance': 1, u'dread': 1, u'circles': 1, u'play': 1, u'experienced': 1, u'pain': 1, u'knew': 1, u'exercised': 1, u'pertinence': 1, u'involving': 1, u'net': 1, u'indicated': 1, u'elizabethans': 1, u'cover': 1, u'musical': 1, u'looked': 1, u'proceedings': 1, u'part': 1, u'professional': 1, u'nineteenth': 1, u'justify': 1, u'painting': 1, u'scream': 1, u'came': 1, u'show': 1, u'chapters': 1, u'defended': 1, u'chandelier': 1, u'velvety': 1, u'debate': 1, u'impact': 1, u'busy': 1, u'resemblance': 1, u'gulf': 1, u'favor': 1, u'gambles': 1, u'explain': 1, u'enough': 1, u'only': 1, u'circle': 1, u'hope': 1, u'meant': 1, u'get': 1, u'coast': 1, u'tribute': 1, u'during': 1, u'spur': 1, u'falling': 1, u'endless': 1, u'prime': 1, u'runs': 1, u'organ': 1, u'artist': 1, u'planned': 1, u'twice': 1, u'activity': 1, u'including': 1, u'ruins': 1, u'art': 1, u'acquired': 1, u'enjoyed': 1, u'all-american': 1, u'edition': 1, u'bare': 1, u'sea': 1, u'seal': 1, u'comfort': 1, u'error': 1, u'federal': 1, u'genius': 1, u'state': 1, u'difficulties': 1, u'injustice': 1, u'progress': 1, u'conditions': 1, u'variability': 1, u'persians': 1, u'sorrow': 1, u'strides': 1, u'lefthanders': 1, u\"pip's\": 1, u'joe': 1, u'contamination': 1, u'singers': 1, u'cities': 1, u\"college's\": 1, u'both': 1, u'prospects': 1, u'cubes': 1, u'successes': 1, u'adventures': 1, u'minds': 1, u'sensitive': 1, u'admit': 1, u'bullhide': 1, u'disappointment': 1, u'forgotten': 1, u\"era's\": 1, u'expense': 1, u'satisfactions': 1, u'period': 1, u'exploit': 1, u'learning': 1, u'throughout': 1, u'trays': 1, u\"it's\": 1, u'satire': 1, u'dangers': 1, u'poet': 1, u'meeting': 1, u'faced': 1, u'senses': 1, u'horsemen': 1, u'quantities': 1, u'offered': 1, u'goddess': 1, u'enterprises': 1, u'assert': 1, u'fund': 1, u'races': 1, u'champions': 1, u'those': 1, u'conception': 1, u'look': 1, u'turtles': 1, u'these': 1, u'tolerate': 1, u'fun': 1, u'aid': 1, u'discoveries': 1, u'remembering': 1, u'real': 1, u'strait': 1, u'danes': 1, u\"hangman's\": 1, u'helped': 1, u'rulers': 1, u'swinging': 1, u'aspects': 1, u'if': 1, u'1815': 1, u')': 1, u'things': 1, u'balloon': 1, u'intrinsic': 1, u'member': 1, u'lyricists': 1, u'speaker': 1, u'gateways': 1, u'dickens': 1, u'elders': 1, u'deference': 1, u'european': 1, u'ball': 1, u'again': 1, u'foundling': 1, u'published': 1, u'delight': 1, u'moving': 1, u'cleaning': 1, u'favorites': 1, u'collar': 1, u'single': 1, u'task': 1, u'theater': 1, u'records': 1, u'i': 1, u'smokies': 1, u'thought': 1, u'solve': 1, u'relief': 1, u'qualities': 1, u'english': 1, u'undertaken': 1, u'spend': 1, u'comprise': 1, u'sinner': 1, u'seasonal': 1, u'ambassador': 1, u'watching': 1, u'guitarist': 1, u'increasingly': 1, u'oak': 1, u'out-of-doors': 1, u'sweeping': 1, u'atomic': 1, u'questions': 1, u'skill': 1, u'northern': 1, u'speed': 1, u'rose': 1, u'regarded': 1, u'character': 1, u'clodhoppers': 1, u'role': 1, u'espoused': 1, u'kingdom': 1, u'dignity': 1, u'early': 1, u'game': 1, u'hitters': 1, u'stylist': 1, u'fortune': 1, u'glove': 1, u'tsunami': 1, u'individuals': 1, u'superiority': 1, u'officer': 1, u'popular': 1, u'become': 1, u'backlog': 1, u'therapeutic': 1, u'parade': 1, u'people': 1, u'pals': 1, u'successfully': 1, u'back': 1, u'dead': 1, u'economic': 1, u'choosing': 1, u'hissing': 1, u'fanfare': 1, u'intellect': 1, u'composers': 1, u'denominations': 1, u'arthur': 1, u'creative': 1, u'leap': 1, u'sherman': 1, u'odor': 1, u'virtues': 1, u'manufacturer': 1, u'chapter': 1, u'about': 1, u'central': 1, u'column': 1, u'newspapers': 1, u'favorite': 1, u'practical': 1, u'range': 1, u'stand': 1, u'letters': 1, u'within': 1, u'intellectual': 1, u'down': 1, u'promise': 1, u'artists': 1, u'crowd': 1, u'distinctions': 1, u'disapproval': 1, u'start': 1, u'four-jet': 1, u':': 1, u'complete': 1, u'offer': 1, u'denoting': 1, u'talents': 1, u'plague': 1, u'bach': 1, u'line': 1, u'places': 1, u'sounds': 1, u'mists': 1, u'attached': 1, u'us': 1, u'oily': 1, u'excitement': 1, u'placed': 1, u'lays': 1, u'bearing': 1, u'promises': 1, u'taste': 1, u'certain': 1, u'suspicions': 1, u'cathedral': 1, u'treaty': 1, u'film': 1, u'memberships': 1, u'upheaval': 1, u'glacier': 1, u'when': 1, u'forests': 1, u'actor': 1, u'actors': 1, u'becomes': 1, u'feare': 1, u'really': 1, u'handicap': 1, u'symbol': 1, u'2:01.1': 1, u'transported': 1, u'gifts': 1, u'gatsby': 1, u'occasion': 1, u'exposure': 1, u'chords': 1, u'starting': 1, u'required': 1, u'time': 1, u'walker': 1, u'requires': 1})}]\n",
      "[{'bad': Counter({u'a': 34, u'was': 20, u'.': 14, u',': 11, u'and': 10, u'of': 9, u'too': 9, u'in': 9, u'good': 8, u'the': 8, u'or': 7, u'be': 6, u'luck': 5, u'one': 5, u'thing': 5, u'to': 4, u'weather': 4, u'with': 4, u'taste': 4, u'is': 4, u'as': 4, u'shape': 3, u'cold': 3, u'pretty': 3, u'not': 3, u'for': 3, u'than': 3, u'had': 3, u'were': 3, u'something': 3, u'nothing': 3, u'guys': 3, u'just': 2, u'feelings': 2, u'condition': 2, u'some': 2, u'are': 2, u'health': 2, u';': 2, u'business': 2, u'by': 2, u'place': 2, u'he': 2, u'judgment': 2, u'almost': 2, u'``': 2, u'that': 2, u\"ain't\": 2, u'ugly': 2, u'it': 2, u\"''\": 2, u'thoroughly': 2, u'when': 2, u'pain': 2, u'off': 2, u'i': 2, u'so': 2, u'things': 2, u'all': 1, u'being': 1, u'indeed': 1, u'results': 1, u'alignment': 1, u'fit': 1, u'race-driver': 1, u'whiskey': 1, u'father': 1, u'grace': 1, u'late': 1, u'dreams': 1, u'has': 1, u'hat': 1, u'into': 1, u'happened': 1, u'coffee': 1, u'publicity': 1, u'very': 1, u'horror': 1, u'squashing': 1, u'early': 1, u'suffering': 1, u\"heat's\": 1, u'him': 1, u'girl': 1, u'--': 1, u'always': 1, u'calling': 1, u'become': 1, u'went': 1, u'because': 1, u'habit': 1, u'burdened': 1, u'whit': 1, u'what': 1, u'slice': 1, u'space': 1, u'avoid': 1, u\"kid's\": 1, u'enough': 1, u\"'ceptin'\": 1, u'outfielders': 1, u'got': 1, u'conditions': 1, u'we': 1, u'however': 1, u'quite': 1, u'incident': 1, u'slum': 1, u'boss': 1, u'on': 1, u'about': 1, u'keep': 1, u'soldier': 1, u'planned': 1, u\"it's\": 1, u'usually': 1, u'equally': 1, u'already': 1, u'froze': 1, u'communication': 1, u'merely': 1, u'whatever': 1, u'variety': 1, u'done': 1, u'been': 1, u'another': 1, u'your': 1, u'better': 1, u'necessarily': 1, u'from': 1, u'temperature': 1, u'would': 1, u'there': 1, u'relations': 1, u'way': 1, u'memory': 1, u'tell': 1, u\"it'd\": 1, u'more': 1, u'succumbed': 1, u'occurred': 1, u'but': 1, u'spades': 1, u'reek': 1, u'case': 1, u'kind': 1, u'whether': 1, u'reportage': 1, u\"that'd\": 1, u'situation': 1, u'piece': 1, u'sense': 1, u'influence': 1, u'itself': 1, u'at': 1, u'have': 1, u'check': 1, u'information': 1, u'needs': 1, u'make': 1, u'delay': 1, u'book': 1, u'detonation': 1, u'day': 1, u'picture': 1, u'play': 1, u'teeth': 1, u'mouth': 1, u'blood': 1, u'such': 1, u'especially': 1, u'light': 1, u'sometimes': 1, u'coincides': 1, u'ultimate': 1, u'martini': 1})}, {'nasty': Counter({u',': 2, u'what': 1, u'a': 1, u'little': 1, u'get': 1, u'did': 1, u'fall': 1, u'whatever': 1, u'hafta': 1, u'down': 1, u'thing': 1, u'you': 1, u'the': 1, u'.': 1})}, {'poor': Counter({u'the': 23, u'a': 17, u',': 14, u'john': 9, u'``': 9, u'and': 9, u'to': 7, u'of': 7, u'are': 6, u'was': 6, u'very': 5, u'were': 4, u'as': 4, u'in': 4, u'boy': 3, u'be': 3, u'or': 3, u'that': 3, u'with': 3, u'is': 3, u'cousin': 3, u'they': 2, u'people': 2, u'george': 2, u'health': 2, u'isaac': 2, u'by': 2, u'policy': 2, u'save': 2, u'circulation': 2, u'all': 1, u'consciousness': 1, u'gontran': 1, u'caused': 1, u'cheap': 1, u'shape': 1, u'including': 1, u'democrats': 1, u'winds': 1, u'bemaddening': 1, u'its': 1, u'believe': 1, u'just': 1, u'reverend': 1, u'sufferer': 1, u'controls': 1, u'actually': 1, u'being': 1, u'only': 1, u'rich': 1, u'folks': 1, u'troubled': 1, u'woman': 1, u'hit': 1, u'watching': 1, u'ones': 1, u'bastards': 1, u'advised': 1, u'not': 1, u'defenseless': 1, u'areas': 1, u\"we'll\": 1, u'stranger': 1, u'success': 1, u'crops': 1, u'communications': 1, u'red': 1, u'where': 1, u'old': 1, u'weight': 1, u'publishers': 1, u'some': 1, u'tee': 1, u'dumb': 1, u'year': 1, u'girl': 1, u'substitute': 1, u'dispersed': 1, u'for': 1, u'hazards': 1, u'ways': 1, u'god': 1, u'harvests': 1, u'providence': 1, u'dr.': 1, u'got': 1, u';': 1, u'espionage': 1, u'substitutes': 1, u'frank': 1, u'tarred': 1, u'alone': 1, u'news': 1, u'both': 1, u'training': 1, u'ol': 1, u'could': 1, u'british': 1, u\"john's\": 1, u'ex-jazz': 1, u'communication': 1, u'after-effects': 1, u'manners': 1, u'sailor': 1, u'simply': 1, u'frequently': 1, u'your': 1, u'devil': 1, u'would': 1, u'area': 1, u'hospital': 1, u'bargaining': 1, u'.': 1, u'territories': 1, u\"boy's\": 1, u'white': 1, u'back': 1, u'farm': 1, u\"john'll\": 1, u'charming': 1, u\"ain't\": 1, u'copy': 1, u'than': 1, u'those': 1, u'throw': 1, u'target': 1, u'this': 1, u\"mother's\": 1, u'i': 1, u'while': 1, u'teachers': 1, u'demonstrate': 1, u'sympathizing': 1, u'my': 1, u'soil': 1, u'formulation': 1, u\"''\": 1, u'his': 1, u'wished': 1, u'if': 1, u'these': 1, u'bastard': 1, u'rather': 1, u'make': 1, u'mr.': 1, u'other': 1, u'which': 1, u'you': 1, u'administrator': 1, u'may': 1, u'such': 1, u'judgment': 1, u'blamed': 1, u'kid': 1, u'attempt': 1, u'sock': 1, u'charges': 1, u'visiting': 1, u'competitors': 1, u'mother': 1, u'uneducated': 1, u'understand': 1})}, {'negative': Counter({u'a': 11, u'the': 11, u'pressure': 7, u',': 5, u'or': 5, u'positive': 5, u'and': 5, u'in': 5, u'by': 4, u'concentrated': 3, u'on': 3, u'this': 3, u'charge': 2, u'small': 2, u'side': 2, u'because': 2, u'for': 2, u'pressures': 2, u'attitudes': 2, u'strong': 2, u'of': 2, u'was': 2, u'is': 2, u')': 2, u'results': 1, u'shake': 1, u'its': 1, u'electricity': 1, u'(': 1, u'to': 1, u'policy': 1, u'overcome': 1, u'them': 1, u'his': 1, u'increased': 1, u'answer': 1, u'using': 1, u'spur': 1, u'association': 1, u'either': 1, u'are': 1, u'home': 1, u'eighteenth-century': 1, u'even': 1, u'tensions': 1, u'particles': 1, u'exercise': 1, u'vs.': 1, u'essentially': 1, u'priorities': 1, u'greatly': 1, u'simple': 1, u'one': 1, u'quality': 1, u'role-experimentation': 1, u'from': 1, u'bears': 1, u'.': 1, u':': 1, u'type': 1, u'head': 1, u'corresponding': 1, u'that': 1, u'explanation': 1, u'but': 1, u'showed': 1, u'sanctions': 1, u'up': 1, u'values': 1, u'can': 1, u'expect': 1, u'share': 1, u'an': 1, u'as': 1, u'warning': 1, u'concentration': 1, u'imitations': 1, u'field': 1, u'sentiments': 1, u'which': 1, u'test': 1, u'produce': 1, u'response': 1, u'identity': 1, u'baker': 1, u'gesture': 1, u\"that's\": 1})}, {'unfortunate': Counter({u'the': 5, u'an': 4, u'as': 4, u'is': 3, u'be': 3, u'that': 3, u'this': 3, u'johnston': 3, u'.': 2, u'to': 2, u'but': 2, u'of': 2, u'people': 1, u'queen': 1, u'some': 1, u'it': 1, u'in': 1, u'was': 1, u'resemblance': 1, u'for': 1, u'habit': 1, u'had': 1, u',': 1, u'been': 1, u'assumption': 1, u'white': 1, u'has': 1, u'establishment': 1, u'``': 1, u'may': 1, u'effect': 1, u'defect': 1, u'most': 1, u'experiences': 1, u'burden': 1, u'world': 1, u'pilot': 1, u'on': 1, u'necessary': 1, u'thought': 1, u'matter': 1})}, {'wrong': Counter({u'the': 25, u'with': 17, u'.': 16, u',': 15, u'was': 15, u'in': 14, u'``': 13, u\"''\": 10, u'and': 9, u'is': 9, u'to': 6, u'anything': 6, u\"what's\": 6, u'nothing': 6, u'what': 5, u\"you're\": 5, u'been': 5, u'something': 5, u'gone': 5, u'on': 5, u'all': 4, u'right': 4, u';': 4, u'had': 4, u'you': 4, u'a': 4, u'go': 3, u'do': 3, u'be': 3, u'were': 3, u'of': 3, u'place': 3, u'that': 3, u'but': 3, u'at': 3, u'always': 2, u'went': 2, u'side': 2, u'goes': 2, u'?': 2, u'by': 2, u'about': 2, u'he': 2, u'word': 2, u'no': 2, u'may': 2, u'man': 2, u'i': 2, u'just': 1, u'being': 1, u'hats': 1, u'find': 1, u'issues': 1, u\"he's\": 1, u'smith': 1, u'going': 1, u'has': 1, u'woman': 1, u'very': 1, u'matters': 1, u'ones': 1, u'nearly': 1, u'they': 1, u'not': 1, u'now': 1, u'bank': 1, u'slavery': 1, u'she': 1, u'car': 1, u'guessing': 1, u'old': 1, u'people': 1, u'some': 1, u'somehow': 1, u'see': 1, u'are': 1, u'thing': 1, u\"'\": 1, u'decision': 1, u'seriously': 1, u'method': 1, u'shows': 1, u'steering': 1, u'never': 1, u'patient': 1, u'responses': 1, u'undo': 1, u'completely': 1, u'response': 1, u'come': 1, u'change': 1, u'both': 1, u'turn': 1, u\"it's\": 1, u'or': 1, u\"can't\": 1, u'one': 1, u'done': 1, u'specifically': 1, u'table': 1, u'tools': 1, u'your': 1, u'discrimination': 1, u'there': 1, u'few': 1, u'way': 1, u':': 1, u'type': 1, u'wrings': 1, u'am': 1, u'it': 1, u'part': 1, u'me': 1, u'made': 1, u'this': 1, u'us': 1, u'turnings': 1, u'behavior': 1, u'joke': 1, u'declare': 1, u'proved': 1, u'played': 1, u'thus': 1, u'direction': 1, u'proven': 1, u'as': 1, u'have': 1, u'any': 1, u'if': 1, u'!': 1, u'entirely': 1, u'rubbed': 1, u'things': 1, u'when': 1, u'regions': 1, u'how': 1, u'though': 1, u'after': 1, u'guys': 1, u'shockingly': 1, u'later': 1, u'congregationalists': 1, u'think': 1, u'thought': 1, u'so': 1, u'far': 1})}, {'inferior': Counter({u'``': 2, u'the': 2, u'decidedly': 1, u'on': 1, u'proved': 1, u'being': 1, u'feel': 1, u'who': 1, u'socially': 1, u',': 1, u'been': 1, u'an': 1, u\"''\": 1, u'to': 1, u'as': 1, u'surface': 1, u'.': 1, u'or': 1, u'man': 1})}, {'awful': Counter({u'an': 5, u'to': 3, u'is': 2, u'in': 2, u'too': 2, u'of': 2, u'the': 2, u\"''\": 1, u\"penny's\": 1, u'at': 1, u'crime': 1, u'its': 1, u'what': 1, u'takes': 1, u'her': 1, u'angry': 1, u'.': 1, u'charge': 1, u'call': 1, u'sorry': 1, u'beating': 1, u'was': 1, u'nice': 1, u'be': 1, u'``': 1, u'good': 1, u'that': 1, u'thieving': 1, u'friday': 1, u'evil': 1, u'boredom': 1, u\"i'm\": 1, u'those': 1, u'he': 1, u'especially': 1, u'hour': 1, u'thing': 1, u'truth': 1, u'having': 1})}]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "\n",
    "words_counter = Counter(bw.lower() for bw in brown.words())\n",
    "total_count = float(len(brown.words()))\n",
    "near_k = 2 # stands for the index distance between seeds and other words, only within the k, the word is considered\n",
    "positive3 = []\n",
    "negative3 = []\n",
    "\n",
    "po_counter = Counter()\n",
    "ne_counter = Counter()\n",
    "\n",
    "def build_dict_for_seeds(seeds):\n",
    "    dicts = []\n",
    "    words = []\n",
    "    for seed in seeds:\n",
    "        seed_word = []\n",
    "        for sent in brown.sents():\n",
    "            sent = [word.lower() for word in sent]\n",
    "#  only the word beside the seed can be considered as co-occurance, only build these words to dictioary and matrix\n",
    "            if seed in sent:\n",
    "                i = sent.index(seed)\n",
    "                if i < near_k:\n",
    "                    seed_word += sent[0:(i+near_k)]\n",
    "                else:\n",
    "                    seed_word += sent[(i-near_k):(i+near_k)]\n",
    "        counter = Counter()\n",
    "        for w1 in seed_word:\n",
    "            counter[w1] +=1\n",
    "        seed_dic = {seed:counter}\n",
    "        words += counter.elements()\n",
    "        ele_dic = seed_dic[seed]\n",
    "        del ele_dic[seed]\n",
    "        dicts.append(seed_dic)   \n",
    "    return dicts, words\n",
    "\n",
    "\n",
    "p_dict, p_words = build_dict_for_seeds(positive_seeds)\n",
    "n_dict, n_words = build_dict_for_seeds(negative_seeds)\n",
    "distict_words = list(set(p_words+n_words))\n",
    "\n",
    "print distict_words\n",
    "print p_dict\n",
    "print n_dict\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"detective's\", u'appetite', u'whose', u'presents', u'under']\n",
      "[u'gontran', u'hats', u'electricity', u'seriously', u'sorry']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for word in distict_words:\n",
    "    pmi_sum = 0\n",
    "    for dict1 in p_dict:\n",
    "        for key1, value1 in dict1.iteritems():\n",
    "            both_count = value1[word]\n",
    "            \n",
    "            if (both_count != 0):\n",
    "                pmi_sum += math.log((both_count/total_count)/((words_counter[key1]/total_count)*(words_counter[word]/total_count)), 2)\n",
    "    for dict2 in n_dict:\n",
    "        for key2, value2 in dict2.iteritems():\n",
    "            both_count = value2[word]\n",
    "            if (both_count != 0):\n",
    "                pmi_sum -= math.log((both_count/total_count)/((words_counter[key2]/total_count)*(words_counter[word]/total_count)), 2)\n",
    "    score = pmi_sum/(psize+nsize)\n",
    "    if score >0.3:\n",
    "        positive3.append(word)\n",
    "    elif score < -0.3:\n",
    "        negative3.append(word)\n",
    "\n",
    "                        \n",
    "print positive3[:5]\n",
    "print negative3[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "9bc3c979d073ce646dc0978890cb63c90ededc0fe4cae7cdf7300f44"
   },
   "source": [
    "<b>Instructions</b>: Now you will test these automatically-produced lexicons against a manually-annotated set. There is a manually-built lexicon (the Hu and Liu lexicon) which is included with NLTK. It has a list of positive and negative words, which are accessed as below. First, investigate what percentage of the words in the manual lexicon are in each of the automatic lexicons, and then, only for those words which overlap and which are <em>not</em> in the seed set, evaluate the accuracy of with each of the automatic lexicons. Discuss the results, mentioning why you think the lexicon which won out did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "signature": "cf88d6c947d813f432a58a1edef528bdca8620384cb5dd232af7f621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129 2588 164 0.313734158562 0.381373415856 0.0241674034777\n",
      "498.0 591.0 77.0 1294 1967 23 0.841709722875 0.988408037094 0.609756097561\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()\n",
    "\n",
    "# remove positive and negative seeds\n",
    "positive1 = [p1 for p1 in positive1 if p1 not in positive_seeds]\n",
    "positive2 = [p2 for p2 in positive2 if p2 not in positive_seeds]\n",
    "negative1 = [n1 for n1 in negative1 if n1 not in negative_seeds]\n",
    "negative2 = [n2 for n2 in negative2 if n2 not in negative_seeds]\n",
    "\n",
    "# get all distinct words\n",
    "lexicon1 = set(positive1 +negative1) \n",
    "lexicon2 = set(positive2 +negative2)\n",
    "lexicon3 = set(positive3 +negative3)\n",
    "man_lexi = set(positive_words + negative_words)\n",
    "\n",
    "# count the number of common words--overlap number\n",
    "cnum1 = 0\n",
    "cnum2 = 0\n",
    "cnum3 = 0\n",
    "for wm in man_lexi:\n",
    "    if wm in lexicon1:\n",
    "        cnum1 += 1\n",
    "    if wm in lexicon2:\n",
    "        cnum2 += 1\n",
    "    if wm in lexicon3:\n",
    "        cnum3 += 1\n",
    "\n",
    "percentage1 = (cnum1)/ float(len(man_lexi))\n",
    "percentage2 = (cnum2)/ float(len(man_lexi))\n",
    "percentage3 = (cnum3)/ float(len(man_lexi))\n",
    "print cnum1, cnum2, cnum3, percentage1, percentage2, percentage3\n",
    "\n",
    "\n",
    "posi_size = float(len(positive_words))\n",
    "nega_size = float(len(negative_words))\n",
    "\n",
    "# only the word is both consider negative or positive at both lexicon is correct\n",
    "pnum_in1 = 0\n",
    "pnum_in2 = 0\n",
    "pnum_in3 = 0\n",
    "nnum_in1 = 0\n",
    "nnum_in2 = 0\n",
    "nnum_in3 = 0\n",
    "# correct word in positive and negative list for each auto-lexicon\n",
    "commonp1 = []\n",
    "commonp2 = []\n",
    "commonp3 = []\n",
    "commonn1 = []\n",
    "commonn2 = []\n",
    "commonn3 = []\n",
    "for pw in positive_words:\n",
    "    if pw in positive1:\n",
    "        pnum_in1 += 1.0\n",
    "        commonp1.append(pw)\n",
    "    if pw in positive2:\n",
    "        pnum_in2 += 1.0\n",
    "        commonp2.append(pw)\n",
    "    if pw in positive3:\n",
    "        pnum_in3 += 1.0\n",
    "        commonp3.append(pw)\n",
    "        \n",
    "try:\n",
    "    for wo in negative_words:\n",
    "        if wo in negative1:\n",
    "            nnum_in1 += 1\n",
    "            commonn1.append(wo)\n",
    "        if wo in negative2:\n",
    "            nnum_in2 += 1\n",
    "            commonn2.append(wo)\n",
    "        if wo in negative3:\n",
    "            nnum_in3 += 1\n",
    "            commonn3.append(wo)\n",
    "except Exception, e:\n",
    "    print e\n",
    "    \n",
    "accuracy1 = (pnum_in1+nnum_in1)/ cnum1\n",
    "accuracy2 = (pnum_in2+nnum_in2)/ cnum2\n",
    "accuracy3 = (pnum_in3+nnum_in3)/ cnum3\n",
    "print pnum_in1, pnum_in2, pnum_in3, nnum_in1, nnum_in2, nnum_in3, accuracy1, accuracy2, accuracy3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss\n",
    "The second lexicon wins because it is done using simialrity between two words in a certain, Which means the meaning\n",
    "of the word in the sentence is similar. So the second lexicon wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "2759c8a43ec5cfa87cd07f42c16ef8c74ad471988ac51b4fee1ec29c"
   },
   "source": [
    "<b>Instructions</b>: Now you will use the lexicons (both manual and automatic) for the main classification problem. Create a function which calculates a polarity score for a sentence based on a given lexicon (i.e. counting positive and negative words that appear in the tweet, and then returning +1 if there are more positive words, -1 if there are more negative words, and 0 otherwise). Then, use this to compare the results of the different lexicons (please convert them to sets!) on the task in the development set, i.e. the accuracy relative to the human-annotated labels. Do the results reflect the quality of the lexicon as indicated by the earlier analysis? How does it compare to the logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "signature": "8b3bedbd64c2ca9a002ae1018eceb428de07cf941d1351dccca62879"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.395844723893\n",
      "0.452706396938\n",
      "0.331875341717\n"
     ]
    }
   ],
   "source": [
    "def compute_polarity(positive_lexicon, negative_lexicon, sentence):\n",
    "    score = 0\n",
    "    for tk in sentence:\n",
    "        if tk in positive_lexicon:\n",
    "            score += 1\n",
    "        if tk in negative_lexicon:\n",
    "            score -= 1\n",
    "    \n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "label1 = []\n",
    "label2 = []\n",
    "label3 = []\n",
    "\n",
    "lenth = len(devlabel)\n",
    "\n",
    "for li in devset:\n",
    "    label1.append(compute_polarity(positive1, negative1, li))\n",
    "    label2.append(compute_polarity(positive2, negative2, li))\n",
    "    label3.append(compute_polarity(positive3, negative3, li))\n",
    "\n",
    "correct_num1 = 0\n",
    "correct_num2 = 0\n",
    "correct_num3 = 0\n",
    "for i in range(lenth):\n",
    "    if devlabel[i] == label1[i]:\n",
    "        correct_num1 += 1\n",
    "    if devlabel[i] == label2[i]:\n",
    "        correct_num2 += 1\n",
    "    if devlabel[i] == label3[i]:\n",
    "        correct_num3 += 1\n",
    "accu1 = correct_num1/float(lenth)\n",
    "accu2 = correct_num2/float(lenth)\n",
    "accu3 = correct_num3/float(lenth)\n",
    "\n",
    "print accu1\n",
    "print accu2\n",
    "print accu3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "The accuracy is the as ealier analysis. Simi-lexicon wins. But the result is not as good as logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "b93c18b028b07adc593487f1f4c0da33e1230121b06ee6300d68de0d"
   },
   "source": [
    "<b>Instructions</b>: Now you should investigate the effect of adding the polarity score (or scores) as a feature in your statistical classifier. You should create a new version of your convert_to_feature_dict function (with a different name) to include the extra feature (or features), do not modify the code in that earlier section directly. Retrain your best logistic regression classifier from the early tuning, test on the development set. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "signature": "764b4b8a168507b6dea50105a68f93ba14616f1cce923b3bfe52eb71"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def polarity_list (dataset):\n",
    "    polarities = []\n",
    "    for tweet in dataset:\n",
    "        polarity = compute_polarity(positive2, negative2, tweet)\n",
    "        polarities.append(polarity)\n",
    "    return polarities\n",
    "\n",
    "def add_polarity(datadicts,polarities):\n",
    "    new_dicts = copy.deepcopy(datadicts)\n",
    "    for i in range(len(new_dicts)):\n",
    "        new_dicts[i]['wd.polarity'] = polarities[i]\n",
    "    return new_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50574084199\n"
     ]
    }
   ],
   "source": [
    "# use the dataset computed before\n",
    "train_pol = polarity_list(trainset)\n",
    "dev_pol = polarity_list(devset)\n",
    "\n",
    "#  user the dictionary computed before\n",
    "train_dictpol = add_polarity(train_dict, train_pol)\n",
    "dev_dictpol = add_polarity(dev_dict, dev_pol)\n",
    "\n",
    "train_datapol, dev_datapol = prepare_for_classification(train_dictpol, dev_dictpol)\n",
    "lg = LogisticRegression(solver='newton-cg', fit_intercept=True)\n",
    "lg.fit(train_datapol, trainlabel)\n",
    "accuracy = lg.score(dev_datapol,devlabel)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My comment\n",
    "With polarity as a feature, the result decreased a little when compare with the original one. Because the polarity accuracy is not so high, so sometimes the feature of polarity can be a bad feature. It will effect the accuracy of the total performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e66c63207f9f85fec6e179a709ad3aa8b20e86030a468b72c7b04cb4"
   },
   "source": [
    "## Error analysis and improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "b435b008310da6d5b5a9b78718e465f87aa900b2c62ee34a95be2a34"
   },
   "source": [
    "<b>Instructions</b>: Using your best logistic regression classifier so far, first write a function to identify errors your classifier is making where the probability of the predicted class and the actual class are fairly close (less than 0.2); you're looking for cases which you have a good chance of getting right with a small improvement. For this, do an 80/20 split of  the training dataset (that is, train on 80% of the data, test on 20%); do not look at examples from the development set or the test set. You should print out the tweet, the correct class, the predicted class, and the probabilities. Don't print all the errors, just use random.sample to select 30 from the full set. Look for general patterns in the errors, and propose a reasonable improvement to your classifier that you think might help with a problem that you are seeing. It could involve, for instance, better preprocessing, the addition of new features, some kind of feature selection, better lexicons or better use of the lexicons, or even a post-processing step. It should not require additional data, unless it involves a small set of words that you can hardcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "signature": "1fbf4e8c1de396a8c67d86d66fc32aa9c30f785f7d4d001167c7925f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  1]\n"
     ]
    }
   ],
   "source": [
    "my_best = LogisticRegression(solver = 'newton-cg',fit_intercept = True)\n",
    "\n",
    "totalnum = len(trainset)\n",
    "train_index = (totalnum/5)*4\n",
    "# divide the train_data to two parts (80/20), 80% part for train, 20% part for test \n",
    "for_train = train_data[:train_index]\n",
    "for_test = train_data[train_index:]\n",
    "trlabel = trainlabel[:train_index]\n",
    "telabel = trainlabel[train_index:]\n",
    "\n",
    "# train my model\n",
    "my_best.fit(for_train, trlabel)\n",
    "\n",
    "# observe the classes order of my_best\n",
    "print my_best.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'i', u'saw', u'her', u'on', u'tuesday', u'by', u'java', u'jazz', u'and', u'i', u'can', u't', u'help', u'it', u'if', u'girls', u'like', u'me', u'lol', u'p'], 1, 0, [0.0048031690617279625, 0.5872565797673238, 0.40794025117094823])\n",
      "([u'army', u'wive', u'until', u'i', u'sleep', u'early', u'and', u'long', u'day', u'tomorrow'], 0, 1, [0.08791034465089265, 0.3649232401829562, 0.5471664151661512])\n",
      "([u'hey', u'what', u'time', u'be', u'we', u'suppose', u'to', u'be', u'at', u'school', u'on', u'monday', u'and', u'what', u'time', u'will', u'it', u'end'], 0, -1, [0.5142392423360287, 0.3848267388397936, 0.10093401882417763])\n",
      "([u'i', u'see', u'a', u'bite', u'of', u'christian', u'in', u'him', u'at', u'1st', u'i', u'wnt', u'him', u'for', u'the', u'role', u'until', u'i', u'saw', u'matt', u'i', u'love', u'him', u'more', u'wen', u'i', u'get', u'2', u'watch', u'white', u'collar'], 1, 0, [0.023647450781871363, 0.516947298156997, 0.45940525106113156])\n",
      "([u'emile', u'heskey', u'score', u'two', u'goals', u'for', u'the', u'newcastle', u'jet', u'in', u'win', u'over', u'the', u'victory', u'herald', u'sun', u'via'], 0, 1, [0.02340374233110279, 0.4416360026798176, 0.5349602549890796])\n",
      "([u'australian', u'directioners', u'do', u'anyone', u'know', u'what', u'time', u'the', u'mtt', u'premire', u'be', u'tomorrow', u'xx'], 0, 1, [0.024254725525561103, 0.45268428942267047, 0.5230609850517683])\n",
      "([u'they', u'be', u'go', u'in', u'on', u'the', u'lakers', u'mike', u'brown', u'may', u'need', u'to', u'take', u'note'], 0, 1, [0.15160425338009062, 0.38754059314659317, 0.46085515347331635])\n",
      "([u'look', u'ahead', u'gov', u'haley', u'will', u'visit', u'honda', u'in', u'timmonsville', u'tomorrow', u'at', u'1', u'30pm', u'she', u'll', u'then', u'visit', u'the', u'governor', u's', u'school', u'in', u'hartsville', u'at', u'3pm'], 0, 1, [0.11935960995453962, 0.43188533961488995, 0.44875505043057046])\n",
      "([u'the', u'wednesday', u'ipad', u'edition', u'be', u'out', u'now', u'include', u'tom', u'segal', u's', u'pricewise', u'advice', u'and', u'brilliant', u'preview', u'for', u'juddmonte', u'international', u'day'], 1, 0, [0.03899976072585307, 0.5428494819010448, 0.4181507573731021])\n",
      "([u'french', u'montana', u'live', u'dc', u'star', u'this', u'saturday', u's', u'o', u'check', u'it', u'out', u'labor', u'day', u'w', u'kend', u'labor', u'day', u'w', u'kend'], 0, 1, [0.016293643537269364, 0.42080212353614116, 0.5629042329265894])\n",
      "([u'tonight', u'dr', u'terrie', u'hale', u'scheckelhoff', u'will', u'be', u'formally', u'instal', u'as', u'the', u'11th', u'head', u'of', u'school', u'welcome', u'to', u'the', u'saint', u'family'], 1, -1, [0.3559452080559952, 0.31771703719387445, 0.3263377547501303])\n",
      "([u'off', u'to', u'to', u'see', u'the', u'shepard', u'fairey', u'show', u'i', u'didn', u't', u'get', u'to', u'see', u'last', u'friday', u'steal', u'space', u'l', u'n', u'd', u'steal', u'space', u'l', u'n', u'd'], -1, 1, [0.26531425776234724, 0.2794823480175657, 0.455203394220087])\n",
      "([u'can', u't', u'wait', u'to', u'see', u'the', u'people', u'of', u'shotton', u'proudly', u'wave', u'their', u'red', u'white', u'flag', u'and', u'wear', u'their', u'national', u'team', u'shirt', u'poland', u'play', u'tomorrow'], 1, 0, [0.003342507983385841, 0.5965356979152044, 0.4001217941014098])\n",
      "([u'i', u'll', u'let', u'kill', u'joke', u'deal', u'with', u'my', u'sunday', u'night', u'blue', u'instead', u'that', u's', u'dave', u'grohl', u'on', u'drum', u'too'], 0, -1, [0.5338678357025319, 0.44141423106269556, 0.024717933234772376])\n",
      "([u'hip', u'hop', u'rumor', u'be', u'maino', u'head', u'to', u'reality', u'tv', u'brooklyn', u'rapper', u'maino', u'may', u'be', u'the', u'latest', u'rapper', u'to', u'be', u'make', u'the'], 0, -1, [0.5182786932963023, 0.45358171196422437, 0.02813959473947339])\n",
      "([u'you', u'and', u'matt', u'and', u'alex', u'no', u'school', u'thurs', u'or', u'fri'], 1, 0, [0.07409645573214957, 0.5419542407192993, 0.38394930354855106])\n",
      "([u'patriots', u'extend', u'lead', u'cruise', u'into', u'4th', u'with', u'38', u'7', u'lead', u'pat', u'pulpit', u'the', u'patriots', u'extend', u'their', u'lead', u'in', u'the', u'3rd'], 1, 0, [0.011048354618992918, 0.5491615823353716, 0.43979006304563556])\n",
      "([u'tryna', u'just', u'chill', u'get', u'a', u'lil', u'tipsy', u'and', u'maybe', u'hit', u'up', u'the', u'mall', u'friday', u'down', u'n', u'down', u'n'], 1, 0, [0.05220541588451048, 0.5333099222772524, 0.4144846618382373])\n",
      "([u'lmaoo', u'gonna', u'have', u'to', u'fight', u'me', u'now', u'think', u'ima', u'wear', u'my', u'warriors', u'gear', u'tomorrow', u'just', u'cause', u'lol'], 1, 0, [0.010943461937170091, 0.558184149198895, 0.4308723888639348])\n",
      "([u'we', u'look', u'like', u'uva', u'there', u'i', u'say', u'it', u'so', u'sad', u'but', u'so', u'very', u'true', u'november', u'24th', u'be', u'go', u'to', u'be', u'a', u'circus'], -1, 0, [0.42616250194608, 0.4410587321842119, 0.1327787658697081])\n",
      "([u'hang', u'at', u'dia', u'wait', u'for', u'my', u'flight', u'to', u'los', u'angeles', u'i', u'm', u'sure', u'the', u'delay', u'have', u'something', u'to', u'do', u'with', u'usc', u'oregon', u'game', u'saturday', u'delay', u'e', u'd', u'delay', u'e', u'd', u'sandy', u'delay', u'e', u'd', u'delay', u'e', u'd', u'sandy'], 0, -1, [0.5347321672915073, 0.4360908287097349, 0.029177003998757937])\n",
      "([u'french', u'press', u'cup', u'of', u'verve', u's', u'los', u'naranjos', u'with', u'a', u'bite', u'of', u'hank', u'williams', u'good', u'morning'], 1, 0, [0.026936064770080446, 0.539694639426443, 0.4333692958034765])\n",
      "([u'heey', u'do', u'you', u'know', u'anything', u'about', u'uva', u's', u'fallll', u'fest', u'loll', u'they', u'invite', u'me', u'so', u'im', u'go', u'this', u'sit', u'but', u'i', u'really', u'dont', u'know', u'what', u'it', u'be', u'loll'], 0, -1, [0.5203586318099577, 0.34116808207837396, 0.13847328611166848])\n",
      "([u'check', u'out', u'sir', u'terry', u'leahy', u'article', u'in', u'saturday', u's', u'telegraph', u'weekend', u'section', u're', u'why', u'he', u'invest', u'in', u'gcse', u'maths', u'resource'], 0, 1, [0.055577833360670376, 0.40874064791993053, 0.5356815187193991])\n",
      "([u'tomorrow', u'be', u'national', u'awareness', u'day', u'wear', u'orange', u'so', u'i', u'know', u'it', u'real', u'freeboot', u'h', u'freeboot', u'h'], 0, 1, [0.029751105744173844, 0.4342207847708407, 0.5360281094849854])\n",
      "([u'premier', u'league', u'team', u'of', u'the', u'week', u'march', u'5', u'gmf', u'take', u'a', u'look', u'at', u'the', u'team', u'of', u'the', u'weeken', u'via'], 0, 1, [0.02438230790844675, 0.4384189266523061, 0.537198765439247])\n",
      "([u'serious', u'talk', u'if', u'you', u'and', u'tyler', u'do', u'a', u'dayz', u'thing', u'i', u'can', u'hook', u'you', u'up', u'with', u'a', u'server', u'skype', u'ocdsupertoaster', u'3', u'cell', u'dinner', u'1st'], 1, 0, [0.05033245961318904, 0.5443553594175343, 0.4053121809692767])\n",
      "([u'do', u'you', u'not', u'even', u'see', u'one', u'for', u'gcse', u'next', u'one', u'be', u'on', u'thursday', u'you', u'should'], 1, 0, [0.11817469946810233, 0.5162465401877947, 0.365578760344103])\n",
      "([u'french', u'press', u'cup', u'of', u'verve', u's', u'los', u'naranjos', u'with', u'a', u'bite', u'of', u'hank', u'williams', u'good', u'morning'], 1, 0, [0.026936064770080446, 0.539694639426443, 0.4333692958034765])\n",
      "([u'many', u'websites', u'go', u'dark', u'today', u'in', u'protest', u'of', u'sopa', u'and', u'pipa', u'you', u'may', u'have', u'notice', u'that', u'some', u'of', u'the', u'websites'], -1, 0, [0.39702642693533435, 0.5807303730516115, 0.022243200013054222])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# error list\n",
    "error = []\n",
    "\n",
    "for i in range(train_index,totalnum):\n",
    "#   get the result using my_best model\n",
    "    my_result = my_best.predict(train_data[i]).tolist()[0]\n",
    "#   get the probabilities of all classes\n",
    "    probabilities = my_best.predict_proba(train_data[i]).tolist()[0]\n",
    "    \n",
    "#   if my_result is not the same as the correct label, find out the probability of right class, calculate the\n",
    "#   difference between my_result probabilty and the right class, if the difference is smaller tha n 0.2,\n",
    "#   add it to error\n",
    "    if my_result != trainlabel[i]:\n",
    "        if trainlabel[i] == 1:\n",
    "            correctpro= probabilities[2]\n",
    "        elif trainlabel[i] == 0:\n",
    "            correctpro = probabilities[1]\n",
    "        else:\n",
    "            correctpro = probabilities[0]\n",
    "        if (max(probabilities) - correctpro) <0.2:\n",
    "            error.append((trainset[i],trainlabel[i],my_result, probabilities))\n",
    "\n",
    "#get randomly 30 samples. print it in a more clear way\n",
    "for y in range(30):\n",
    "    print random.choice(error)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "By observe the printed out samples, we can find that in the original tweets, the occurrence of some meaningless words take certain weights. Words like \"i\", \"you\", \"what\" and digitals, this words is helpless in classifying. The useless words are mostly prepositions, pronoun, conjunctions, modals and determiners. So We should remove these words in preprocess. This can be a improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "92b57c864e0ef5e2c69ce85fc40a024681d1220bf5d2bd681bc6f149"
   },
   "source": [
    "<b>Instructions</b>: Now implement that improvement, and then investigate its effect <em>in the development data</em>. Obviously, different improvements may involve different amounts of effort; if your improvement is fairly simple, we expect that you will do a more in-depth analysis, testing possible variations. You can also do multiple related improvements. Students who put extra effort into this may get few extra points that can offset any mistakes on other parts of the assignment, though we do not recommend you spend extra time on this before the other parts of the assignment are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "signature": "1ca4be68b2c405f4ba53c54ae3030c9865169f0544812c32705635b5"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger\n",
    "train_sents = brown.tagged_sents(tagset=\"universal\")\n",
    "\n",
    "# train tagger \n",
    "default_tagger = DefaultTagger(\"NN\")\n",
    "unigram_tagger = UnigramTagger(train_sents,backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents,backoff=unigram_tagger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tag each token and remove useless\n",
    "\n",
    "# cause I'm using universal tagset, so some meaning less words may not be well taged, so I remvoe them manully, include\n",
    "# single characters\n",
    "def improvement(dataset):\n",
    "    uncovered_words = ['be', 'how', 'will', 'would', 'can', 'may','es']\n",
    "    after_tag = []\n",
    "    for tweet in dataset:\n",
    "        tagged = bigram_tagger.tag(tweet)\n",
    "        tokens = []\n",
    "        for t in tagged:\n",
    "            if ((t[1] not in ['PRON', 'NUM', 'DET', 'ADP', 'CONJ']) and (len(t[0]) != 1)) :\n",
    "                tokens.append(t[0])\n",
    "        after_tag.append(tokens)\n",
    "    return after_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "6ac4b4f552a5394e20aef802baf625642da4a60fcbf0a3304a6a14aa"
   },
   "source": [
    "## Final testing and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "signature": "e2317348e5598fb29c4280dd5d015c20b4aa817bd02455d79cd0bec7"
   },
   "source": [
    "<b>Instructions</b>: When the final test set has been released, you should start with your best classifier from your work up to this point, and do a final test of all major options, including at least one from each of the first four sections of the assignment (that is, at least one preprocessing option, at least one tuning parameter/classifier type, at least one lexicon, and your improvement), in this new dataset. You don't need to explore every possible combination (in fact, you shouldn't), but you should make a convincing case that you have probably found the best combination given the possibilities you have implemented. It's okay if you find discrepancies between the best classifier on the test set and development set, just be sure to mention them. In a final discussion (which should be at least 500 words), you should include at least one bar graph of accuracy across various options, and at least one table which reports precision, recall, and F-score for each label as well as the macroaveraged F-score (all figures and tables should be generated inline by your code, using matplotlib). Please conclude your discussion by discussing what you have learned, and mentioning any other ideas for improving performance of this system that you may have.\n",
    "\n",
    "Note that you may have to direct matplotlib to display the figures inline, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "signature": "84721d1ef62e202076d184eb77b602ff3ac6816effde9d17be3de157"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "my_bestclf = LogisticRegression(solver = 'newton-cg',fit_intercept = True)\n",
    "testset, testlabel = preprocess_file(u'test.json')\n",
    "improved_testset = improvement(testset)\n",
    "\n",
    "testpol = polarity_list(testset)\n",
    "imptestpol = polarity_list(improved_testset)\n",
    "\n",
    "test_dict = convert_to_feature_dicts(testset,1,0)\n",
    "imtest_dict = convert_to_feature_dicts(improved_testset,1,0)\n",
    "test_dictpol = add_polarity(test_dict, testpol)\n",
    "imtest_dictpol = add_polarity(imtest_dict, imptestpol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "improved_trainset = improvement(trainset)\n",
    "imptrainpol = polarity_list(improved_trainset)\n",
    "imtrain_dict = convert_to_feature_dicts(improved_trainset,1,1)\n",
    "imtrain_dictpol = add_polarity(imtrain_dict, imptrainpol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data,test_data = prepare_for_classification(train_dict, test_dict)\n",
    "imtrain_data,imptest_data = prepare_for_classification(imtrain_dict, imtest_dict)\n",
    "train_datapol,test_datapol = prepare_for_classification(train_dictpol, test_dictpol)\n",
    "imtrain_datapol,imptest_datapol = prepare_for_classification(imtrain_dictpol, imtest_dictpol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess, improved, add polarity and best model: 0.489762\n",
      "Preprocess, improved, add polarity and default model: 0.490315\n",
      "Preprocess, improved, and best model: 0.490869\n",
      "Preprocess, add polarity and best model: 0.483675\n",
      "Preprocess and best model: 0.485888\n",
      "Preprocess and default model: 0.485888\n"
     ]
    }
   ],
   "source": [
    "# with all options\n",
    "my_bestclf.fit(imtrain_datapol,trainlabel)\n",
    "final1 = my_bestclf.score(imptest_datapol, testlabel)\n",
    "\n",
    "# use default lg model\n",
    "model = LogisticRegression()\n",
    "model.fit(imtrain_datapol, trainlabel)\n",
    "final2 = model.score(imptest_datapol, testlabel)\n",
    "\n",
    "# don't use polarity \n",
    "my_bestclf.fit(imtrain_data, trainlabel)\n",
    "final3 = my_bestclf.score(imptest_data, testlabel)\n",
    "\n",
    "# don't use improvement\n",
    "my_bestclf.fit(train_datapol, trainlabel)\n",
    "final4 = my_bestclf.score(test_datapol, testlabel)\n",
    "\n",
    "#only use my tuned best model\n",
    "my_bestclf.fit(train_data, trainlabel)\n",
    "final5 = my_bestclf.score(test_data, testlabel)\n",
    "\n",
    "#only use original \n",
    "model.fit(train_data,trainlabel)\n",
    "final6 = model.score(test_data, testlabel)\n",
    "\n",
    "print \"Preprocess, improved, add polarity and best model: %f\" % final1\n",
    "print \"Preprocess, improved, add polarity and default model: %f\" % final2\n",
    "print \"Preprocess, improved, and best model: %f\" % final3\n",
    "print \"Preprocess, add polarity and best model: %f\" % final4\n",
    "print \"Preprocess and best model: %f\" % final5\n",
    "print \"Preprocess and default model: %f\" % final6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XucFNWZ//HPwzCoKBi8ADIqiOEmdwzEbLIL0XiJRnSR\nxZEkGDXRDYtRYqK42V3RmETMxhhlTdQoqyYyIYoC+QVvhAlmEwWDeAURkbugCN5QrvP8/qjTQ03T\nPVMwXTNDz/f9evVrqk6dqnNOdU8/XefUxdwdERGRurRo7AqIiMj+QQFDREQSUcAQEZFEFDBERCQR\nBQwREUlEAUNERBJRwJBmw8zam9k8M3vfzH6aIP+FZvZ0bP5DM+sSpg80s1lm9p6Z/S6k3Whm75jZ\nurTa0NSZ2RQzu6GW5dX7MIWy/2hmX09j2xJp2dgVkIZjZpVAP6CDu+9o5Oo0hkuBt9390L1Yp/pC\nJXdvE0sfCRwJtHN3N7NjgO8Cx7j7uwWp7V4ws7nAA+5+b0OXvTey9uE+M7PrgOPdfUxs22cWYtuS\nn44wmgkz6wwMAd4Ghjdw2SUNWV4tOgOvFnBbS333la+dgY37GizMzApUL5H0uLtezeAF/CcwA/h3\nYFbWsgOBnwErgM3APOCAsOwLwP+F9JXAmJA+F7g4to0Lgadj81XAWGAp8EZIuxVYBbwPLAC+EMvf\nItRtGfBBWF4GTAb+O6u+M4Ar8rTzH4D5ob7PAp8L6VOA7cC2sP2Tc6x7GDAz1O8Z4AZgXlabugIT\nw3a2h21dCnwM7Azz94b8J8X23fPA0Ni25gI3An8BtoTttgXuAdYBq4EfAhbfv8BPgU3AG8DpYdmN\noeyPQ/m35dk3+d7LtsD9RD8m3gR+kPW+/gW4Jaz3etjH3wjv5frMdmL7+ZfAE6Euc4Fjs/dhLO9k\n4A8h79+A42J5c35egNPD/t8GfAg8n/2ZBAz4D6LP9Hrgf4G2YVnnUI8xYT+8Dfx7Y/+P7g+vRq+A\nXg30Rkf/6KOBbuGL7sjYsv8B/gR0DP9oJwGlwLHhH3kUUAK0A/qFdXIFjOwv18eBQ9kdfEYDnyIK\nDuOBt4BWYdn3gReAT4f5vqG8wcCa2HYPBz4CjsjRxnZEX6ajQxnlYb5dWD4FuKGWfVQRXgcCvYE1\nWW3aFfuyuw64P7ZsKLAqNt8J2MjuL/VTwvzhsf23AugZ6toSeAS4I5R/BFHQ+lZs/24DLg7v0b8C\na2Pl1Xg/crSttvfy/lB26/Bl+hpwUazc7URfrkYUxFYDt4fPyKlhu61j+/h94PNh+a3U/CER34dT\ngHeAE8M++A3wYCxvbZ+XGvs/ex+E/bQ0tKc18HAmP7sDxp1AK6Ju2q1Aj8b+P23qr0avgF4N8CZH\nvyw/Bg4J888TfqGHL4GPgT451psAPJxnm0kCxtA66rUJ6BumlwBfyZPvFeCUMP1vwB/y5Psa8ExW\n2l/Z/Us6b8AIX0rbgW6xtB/laFPSgHE1cF9WGY8BX4/tv4mxZe3Dl9YBsbRy4E+x/bs0tuygUJ/2\nud6PpO9laPe2+Jcl0RFTvNzXYsv6EH3pHxFL28ju4DOFml/6BxMd/ZTl2IdTgLtieb8MvJrw81JX\nwHgK+NfYsu7h/W1BFDB2AUfFlj8LjEr7f3F/f2kMo3kYAzzh7h+F+d8TfRFA9Ev2AGB5jvWOIer6\n2Fdr4jNm9j0ze9XMNpvZZqKukCNiZeWqA8ADRMGA8PeBPPk6EXUxxK0k6tqqy5FEv7zjdc7e1t7o\nDIwys03htZnoV3fHWJ7VWflLgbdi+X/F7v0DUdcKAO7+SZg8JGF98r2XRxAd3ayKpWXvsw2x6U9C\n+Ruz0uL1qG6Xu28h+qLvlKde62PTH8e3U8fnpS7Zn4WVRO3sEEuLt6tG2ZKbzpIqcmZ2IFE3RAsz\neysktwI+ZWZ9gZeJftkeD7yUtfpqooHyXLYQHepndMyRx2P1+AJRt9MX3f3VkLaJ6AgnU9bx5B6U\nfgB4ycz6EXXhPJqnTuuA87LSjgVm58kf9w7RL+FjiLoyMuvuq9VEv4AvqyWPx6ZXE70Ph3v4ybuX\n6lon33u5EdhBFLCWhLTOwNp9qEPGMZkJMzuEaGxor7ZnZv9I7Z+Xutq7jqgdGZ2J2rkhXj/ZOzrC\nKH7/TPRF2AvoH169iAYyx4QvpynALWZ2lJm1MLOTzKwU+C1wipmNNLMSMzvMzPqH7S4CRpjZQWb2\naeCSOurRhugf9l0za2Vm/xXSMn4N/DBsCzPra2btANx9LfB3osDxsLtvy1PGH4FuZlYe6nt+aOsf\n6tpJ7l4FTAcmhjadwO6jsH3xG+BsMzst7NMDzWyomeX8pe3u64kGin9uZm0s0tXM/ilheRuIBs7z\nyflehnZPA35kZoeEs+nGk/8oDnZ/aedzppn9g5m1Ihrz+Ju77+21KYdQ++dlA9CllrPLpgLjzaxL\nCFo/AipCe5O0QXJQwCh+Y4jO2lnr7m9nXkRnp3zVzFoA3yM6ulgAvAvcBLRw99XAmWH5JqKxj35h\nuz8n+odeTxRwfpNVbvYvwMfDaynRmTgfU7NL5haiL64nzOx9ogByUGz5fUT95/fna6i7bwK+Euq7\nMfw9K6TnqlO2y4m+lN4C7g2v2tqUl7uvAc4hOvPrHaIuke+x+38u17bGEB39vUq0v39P7iO3XPX5\nBfAvZvaumd2aoz61vZffIXo/lhOdIfcbd5+SsNzseQceJDqT7F1gILu7E3Otm09dn5ffE33pv2tm\nz+XY9r1EQW8eUVfcx0TtTNIGycP27eh3LwowO4PoTIkWwD3uPilr+VCi0yQz/dfT3f3G2tYNvzx/\nR3SYuYJosOr9VBsijSp0af3G3bs0dl1EmqtUjzDCr9fJROdN9wYuMLOeObLOc/dB4XVjgnUnAE+5\new+i00GvTbMd0rhC99iVwN2NXReR5iztLqkhwOvuvtKjW1FUEB2mZ8vVn1jbuucQdVEQ/p5b2GpL\nUxF+JGwmOrvlF41cHZFmLe2AUUbNfsc15D7F8XNmtsjM/l8YbKxr3Q7uvgGqBwvbF7ba0lS4+xJ3\nP8Td/zF2WrCINIKmcFrt34luHfCxmX2Z6JTJ7nu5DQ1YiYikLO2AsZaa57IfTdb52PFfje4+28zu\nMLPD6lh3vZl1cPcNZtaR6F4wezAzBRIRkX3g7nsMFaTdJbUA+LSZdQ7nZJcT3dytmpl1iE0PITpz\na1Md684kuvkZROfKz8hXgX25/P26665r9Evw03ypffvvq5jbpvY1nVc+qR5huPsuMxtHdEFS5tTY\nxWZ2WbTY7wJGmtm3ic7p/wQ4v7Z1w6YnAdPM7GKi89tHpdkOERFpgDEMd38M6JGVdmds+n+I7paa\naN2Qvgn4UmFrKiIitdGV3jkMGzassauQKrVv/1XMbQO1r6lL/UrvxmRmXsztExFJg5nhjTDoLSIi\nRUIBQ0REElHAEBGRRBQwREQkEQUMERFJRAFDREQSUcAQEZFEFDBERCQRBQwREUlEAUNERBJRwBAR\nkUQUMKTJeeyxx+jZsyfdu3dn0qRJefMtWLCA0tJSpk+fXp32i1/8gr59+9K3b19uu+226vSHHnqI\nPn36UFJSwsKFC/fY1qpVq2jTpg233HJLYRsjUkQUMKRJqaqqYty4cTz++OO88sorTJ06lSVLluTM\nN2HCBE4//fTqtFdeeYV77rmH5557jkWLFjFr1iyWL18OQN++fXnkkUcYOnRoznKvuuoqzjzzzHQa\nJVIkFDCkSZk/fz7dunWjc+fOlJaWUl5ezowZez5Q8fbbb2fkyJG0b9++Om3x4sV89rOf5YADDqCk\npIShQ4dWH3306NGDbt265Xya2IwZM+jatSu9e/dOr2EiRUABQ5qUtWvXcswxx1TPH3300axdW+Mx\n8Kxbt45HH32Ub3/72zUCQJ8+fXj66afZvHkzH3/8MX/84x9ZvXp1reVt2bKFm2++ufrRmSKSX+pP\n3BMptCuvvLLG2Ebmi75nz55cc801nHrqqRxyyCEMHDiQkpKSWrc1ceJExo8fT+vWrWtsS0T2pIAh\nTUpZWRmrVq2qnl+zZg1lZWU18jz33HOUl5fj7mzcuJHZs2dTWlrK8OHDueiii7jooosA+MEPflDj\naCWXZ599locffpirr76azZs3U1JSwkEHHcTYsWML3ziR/ZwChjQpgwcPZtmyZaxcuZKjjjqKiooK\npk6dWiNPZiAb4KKLLuLss89m+PDhALzzzjsceeSRrFq1ikceeYRnnnlmjzLiRxHz5s2rnr7++utp\n06aNgoVIHgoY0qSUlJQwefJkTjvtNKqqqrjkkkvo1asXd955J2bGpZdeWiO/Wc2nSJ533nls2rSJ\n0tJS7rjjDtq2bQvAo48+yuWXX87GjRv5yle+woABA5g9e3aDtUukGOiZ3iIiUoOe6S0iIvWigCEi\nIokoYIiISCIKGCIikogChoiIJKKAISIiiaQeMMzsDDNbYmZLzeyaWvINNrMdZjYilnaFmb0UXlfE\n0q8zszVmtjC8zki7HSIizV2qF+6ZWQtgMnAKsA5YYGYz3H1Jjnw3AY/H0noDlwCfAXYCj5nZLHfP\nXOZ7i7vr4QUiIg0k7SOMIcDr7r7S3XcAFcA5OfJdDjwEvB1L6wU86+7b3H0X8GdgRGz5HheViIhI\netIOGGVA/P7Sa0JaNTPrBJzr7r+kZhB4GfhHM2tnZq2BM4H4neTGmdkiM/u1mR2aTvVFRCSjKQx6\n3wrExzYMIHRbTQKeBP4IPA/sCnnuALq6+wBgPaCuKRGRlKV988G1wLGx+aNDWtxngAqL7iJ3BPBl\nM9vh7jPdfQowBcDMfkQ4WnH3d2Lr3w3MyleBiRMnVk8PGzaMYcOG7WtbRESKUmVlJZWVlXXmS/Xm\ng2ZWArxGNOj9FjAfuMDdF+fJPwWY5e7Tw/yR7v6OmR0LPAac5O4fmFlHd18f8owHBrv76Bzb080H\nRUT2Ur6bD6Z6hOHuu8xsHPAEUffXPe6+2Mwuixb7XdmrZM0/bGaHATuAse7+QUi/2cwGAFXACuCy\n1BohIiKAbm8uIiJZdHtzERGpFwUMERFJRAFDREQSUcAQEZFEFDBERCQRBQwREUlEAUNERBJRwBAR\nkUQUMEREJBEFDBERSaQoA0bmsbAAkyZNyptvwYIFlJaWMn369Oq0n/zkJ/Tu3Zt+/frx1a9+le3b\ntwNQXl7OoEGDGDRoEMcddxyDBg2qsU63bt3o1asXTzzxRFrNEhFpVEUXMGKPhT0dYOrUqSxZsmSP\nfFVVVUyYMIHTTz+9Om3lypXcfffdPP/887z44ovs3LmTiooKACoqKli4cCELFy7kvPPOY8SI6OF/\nixcvZtq0aSxevJjZs2czduxYdP8qESlGRRcwiD0WFqIjgxkzZuyR6fbbb2fkyJG0b9++Oq1t27a0\natWKLVu2sHPnTj7++GM6deq0x7rTpk1j9OjobuozZsygvLycli1b0qVLF7p168b8+fNTapqISOMp\nxoBR47GwRx99NGvX1nxm07p163j00Uf59re/XeNooF27dlx11VUce+yxlJWV8alPfYovfelLNdZ9\n+umn6dixI127dgVg7dq1HHPM7ifHlpWV7VGeiEgxKMaAUacrr7yyxthGJmgsX76cn//856xcuZJ1\n69bx0Ucf8eCDD9ZYd+rUqVxwwQUNWt9sjz32GD179qR79+4FG6OB6KirV69e9O3blwkTJtTY1qpV\nq2jTpg233KKn4Yo0V2k/orUx1Hgs7Jo1aygrK6uR4bnnnqO8vBx3Z+PGjcyePZvS0lK2bt3K5z//\neQ477DAARowYwV//+tfq7qddu3Yxffp0Fi5cWL2tsrIyVq+uPqDJWV4hVVVVMW7cOObMmUOnTp0Y\nPHgw55xzDj179twjX74xmiVLltCqVSvOP/98KioqGDNmDHPnzmXWrFm89NJLtGzZko0bN9bY3lVX\nXcWZZ56ZWrtEpOkrxiOMBcCnzawzRIPVw4cPr5Fh+fLlLF++nDfffJORI0dyxx13MHz4cHr06MEz\nzzzD1q1bcXfmzJlDr169qtd78skn6dWrV41xjeHDh1NRUcH27dt58803WbZsGUOGDEmtcfPnz6db\nt2507tyZ0tLSgo3R/OpXv2LChAm0bBn9hjjiiCOq15sxYwZdu3ald+/eqbVLRJq+ogsY7r4LyDwW\nlvLycnr16sWdd97JXXdlPxE2erJURv/+/RkzZgwnnngi/fv3x9259NJLq5f/7ne/26M76oQTTmDU\nqFGccMIJnHnmmdxxxx01tllo2WMmhRqjWbp0KfPmzeOkk07ii1/8Is899xwAW7Zs4eabb+a6667T\n2V8izVwxdknh7o8BPczMM33xl12W+7Hf9957b43573//+3z/+9/PmXfKlCk506+99lquvfbafa9w\ngSUZozn00EMZOXIkDz74IKNHj2bnzp1s3ryZZ555hgULFjBq1CiWL1/OxIkTGT9+PK1bt66xLRFp\nfooyYBSzsrIyVq1aVT1fqDGao48+uvraksGDB1NSUsK7777Ls88+y8MPP8zVV1/N5s2bKSkp4aCD\nDmLs2LEN12gRaRIUMPYzgwcPZtmyZaxcuZKjjjqKiooKpk6dWiPP8uXLq6cvuugizj77bIYPH84L\nL7zAD3/4Q7Zu3coBBxzAnDlzGDx4MADnnnsuf/rTnxg6dChLly5l+/btHH744cybN696W9dffz1t\n2rRRsBBpphQw9jMlJSVMnjyZ0047jaqqKi655JLqMRozqzHmAvnHaEpKShg4cGB1/osvvpiLL76Y\nvn37csABB3D//fc3aLtEZD/g7kX7ipon+6sOHTo70CCvDh06N3Zzi8Ls2bO9R48e3q1bN7/pppvy\n5ps/f763bNnSH3744eq0H//4x37CCSd43759ffTo0b5t2zZ3d584caKXlZX5wIEDfeDAgT579uzq\nbQwYMMAHDBjg/fr184qKinQb14yE7849vlPNi3gQ08y8mNtX7KKjo4Z6/0wD+vVUVVVF9+7da1wj\nVFFRkfMaoVNPPZWDDjqIiy++mBEjRrBy5Uq++MUv1rhG6KyzzmLMmDHVXaHf/e53a2xn69attGrV\nihYtWrB+/Xr69OnDhg0bKCkpachmFyUzw933ON2z6E6rFZHGkdY1QpD77LwDDzyQFi2ir7BPPvmE\nQw89VMEiZQoYIlIQaV0jBDB58mQGDBjAN7/5Td57773q9Pnz59OnTx/69Omj29Y0AAUMEWkw+3If\nt7Fjx7J8+XIWLVpEx44dueqqq6rXHzJkCC+//DILFy7kiiuu4IMPPmjYBjUzOktKRAoirWuEjjzy\nyOr1v/Wtb3H22WfvUXaPHj04/vjjef311znxxBNTaqHoCENECiJ+jdD27dsLdh+39evXV68/ffp0\n+vTpA8CKFSvYtWsXEN1Yc9myZXTr1q2BWts8pX6EYWZnALcSBad73D3n/bjNbDDwV+B8d58e0q4A\nvhmy3O3ut4X0dsDvgM7ACmCUu7+fZjtEpHZpXSN09dVXs2jRIlq0aEGXLl248847AfjLX/7CTTfd\nRKtWrSgtLeWuu+6ibdu2DdfgZijV02rD41KXAqcA64juJFvu7kty5HsS+AS4192nm1lvYCowGNgJ\nPAZc5u7LzWwS8K6732xm1wDt3L3mAxzQabX7O51WK9I4Guu02urHpbr7DqACOCdHvsuBh4C3Y2m9\ngGfdfZtHd6D9MzAiLDsHuC9M3wecm0blRQotrYdfAfzsZz+jRYsWbNq0CYCnnnqKz3zmM/Tv35/B\ngwczd+7cdBrVjDT79y/X1XyFegHnAXfF5r8G3JaVpxMwN0xPAUaE6Z7AEqAd0Jqou+oXYdnmrG1s\nylP+vl7oKE0A4OAN9Er/s7Jr1y4//vjjfcWKFb59+3bv37+/L168OGe+k08+2c8666zqK6FXrFjh\nxx13XPXVz6NGjfL77ruvep3Vq1f76aef7l26dPF3333X3d0XLVrkb731lru7v/zyy15WVpZ2E4ta\nc3r/yHOld1M4S+pW4JrYvAG4+5LQ9fQk8BHwPLArzzby9iVMnDixenrYsGEMGzasfrUV2UfxC9uA\n6gvbsq+EzlzYtmDBguq0+IVtLVq02OPCtvHjx/PTn/60xiBz//79q6d79+7N1q1b2bFjB6WlpWk1\nsagV8/tXWVlJZWVlnfnSDhg1HpcKHB3S4j4DVFjUYX0E8GUz2+HuM919CtFRB2b2IyDzLNT1ZtbB\n3TeYWUdqdmXVEA8YIo0p14Vt8+fPr5Enc2Hb3LlzayyLX9jWunVrTjvttOoL22bOnMkxxxxD3759\n85b90EMPMWjQIAWLeijm9y/7x/T111+fM1/aYxjVj0s1s1ZAOTAznsHdu4bXcUTjGGPdfSaAmR0Z\n/h4L/DPwYFhtJvCNMH0hsOf9B0T2Q3t7Ydsnn3zCj3/84xr/4Jl1Ml555RWuvfbanE+clMIq9vcv\n1SMMd99lZpnHpWZOq11sZpdFiz17D2R3LT1sZocBO4gCSeYyzknANDO7GFgJjEqvFSKFkcaFbf36\n9WPFihXVjxRes2YNJ554IvPnz6d9+/asWbOGESNG8MADD9ClS5eGbG7R0fuHbm8uTRdFNui9c+fO\n6kHTbdu2ef/+/f3VV1/Nm/8b3/hG9aDpokWLvE+fPv7JJ594VVWVX3jhhT558uQ91unSpYtv2rTJ\n3d3fe+8979+/vz/yyCPpNKiZaU7vH3kGvXWlt0gDiV/Y1rt3b8rLy6svbMvV3ZDvwrbMr9HsC+Ey\n63jo0pg8eTJvvPEGN9xwAwMHDmTQoEFs3LgxvQYWOb1/KV+419h04d7+TRfuiTQOPQ9DRETqRQFD\nREQSUcAQEZFEFDBERCQRBQwREUlEAUNERBJRwBARkUQUMEREJBEFDBERSUQBQ0REEqkzYJjZ5WbW\nriEqIyIiTVeSI4wOwAIzm2ZmZ1j8jloiItJsJLr5YAgSpwEXET0hbxrRsy3eSLd69aObD+7fdPNB\nkcZRr5sPhm/d9eG1E2gHPGRmNxe0liIi0mTVeYRhZlcAY4CNwK+BR919h5m1AF539+PTr+a+0RHG\n/k1HGCKNI98RRpJHtB4GjHD3lfFEd68ys68UqoJpacghlw4dOrN+/YoGK09EpCElOcI4CXjF3T8M\n822BXu7+bAPUr17MzBvuFyroV2ph6QhDpHHUZwzjl8BHsfmPQpqIiDQjSQJGjYEAd68iWVeWiIgU\nkSQBY7mZfcfMSsPrCmB52hWTunXs2AUza7BXx45dGrvJItKIkoxhtAduA04m6lCeA1zp7m+nX736\nKfYxjIbt44fibp/GMEQy8o1hJLpwb3+lgFHwEou4fQoYIhn7fFqtmR0IXAL0Bg7MpLv7xQWtoYiI\nNGlJxjAeADoCpwN/Bo4GPkyzUiIi0vQkGcN43t0HmtmL7t7PzEqBp939pIap4r5Tl1TBSyzi9qlL\nSiSjPtdh7Ah/3zOzPsChQPtCVk5ERJq+JAHjrvA8jP8AZgKvApOSFhBuib7EzJaa2TW15BtsZjvM\nbEQs7Voze8XMXjSz35pZq5B+nZmtMbOF4XVG0vqIiMi+qXXQO9xg8AN33wzMA7ruzcbD+pOBU4B1\nRM/VmOHuS3Lkuwl4PJbWGfgW0NPdt5vZ74By4P6Q5RZ3v2Vv6iMiIvuu1iOMcFX31fXY/hCiO9qu\ndPcdQAVwTo58lwMPAfFrOz4AtgMHm1lLoDVR0MnQg5xEmihdVFqcknRJPWVm3zOzY8zssMwr4fbL\ngNWx+TUhrZqZdQLOdfdfEgsC4ajmZ8AqYC3wnrs/FVt1nJktMrNfm9mhCesjIg1gw4aVRCcsNMwr\nKk/SluSeUOeHv/8WS3P2snuqFrcC8bENAzCzrsB4oDPwPtEDm0a7+4PAHcAN7u5mdiNwC9G1IjlM\njE0PCy8REcmorKyksrKyznypXukdbo0+0d3PCPMTiB7gNymWJ3NfKgOOALYAlxJdJHiqu38r5Ps6\n8Fl3H5dVRmdglrv3y1G+TqstbIlF3D6dVltIxf7ZLHb1udJ7TK50d78/V3qWBcCnw5f6W0SD1hdk\nbaf6SMXMphB9+c80s/7Af4YrzbcRDZwvCPk6uvv6sNoI4OUEdRERkXpI0iU1ODZ9INEX90J2n62U\nl7vvMrNxwBNE4yX3uPtiM7ssWux3Za8SW/cFM7sf+DuwC3geyOS/2cwGAFXACuCyBO0QEZF62Osu\nKTP7FFCR6WZqytQlVfASi7h96tIopGL/bBa7+lzpnW0LcFz9qyQiIvuTJGMYs9j9U6EFcAIwLc1K\niRS7jh27NOipoB06dGb9+hUNVl6xa67vX5KbDw6Nze4EVrr7mlRrVSDqkip4iUXcvmJuG6h9BS6t\nGbRvn86SIrpw7i133xo2dJCZdXH3FQWuo4iINGFJxjB+T3Q2UsaukCYiIs1IkoDR0t23Z2bCdKv0\nqiQiIk1RkoDxjpkNz8yY2TnAxvSqJCIiTVGSQe/jgd8CnULSGmCMuy9LuW71pkHvgpdYxO0r5raB\n2lfg0ppB+3INeie+cM/MDgFw948KXLfUKGAUvMQibl8xtw3UvgKX1gzat08X7pnZj83sU+7+kbt/\nZGbtwh1iRUSkGUkyhvFld38vMxOeU3FmelUSEZGmKEnAKDGzAzIzZnYQcEAt+UVEpAgluXDvt8Cc\ncOtxA74B3JdmpUREpOmpM2C4+yQzewH4EtEoz+NET8ETEZFmJOndajcQBYt/AU4GFqdWIxERaZLy\nHmGYWXeip+OVA28T3Q7E3P2LDVQ3ERFpQmrrkloC/AE4zd1XA5jZdxukViIi0uTU1iU1AvgYmGdm\nvzKzk4kGvUVEpBlKcmuQg4FziLqnTiZ6lvcj7v5E+tWrH13pXfASi7h9xdw2UPsKXFozaF+9bg0S\nNtKOaOD7fHc/pYD1S4UCRsFLLOL2FXPbQO0rcGnNoH31Dhj7GwWMgpdYxO0r5raB2lfg0ppB+/bp\nXlIiIiKggCEiIgkpYIiISCIKGCIikogChoiIJKKAISIiiShgiIhIIqkHDDM7w8yWmNlSM7umlnyD\nzWyHmY3GbknYAAANvUlEQVSIpV1rZq+Y2Ytm9lszaxXS25nZE2b2mpk9bmaHpt0OEZHmLtWAYWYt\ngMnA6UBv4AIz65kn301Ez9rIpHUGvgUMdPd+RDdKLA+LJwBPuXsP4E/AtWm2Q0RE0j/CGAK87u4r\n3X0HUEF0X6pslwMPEd1GPeMDYDtwsJm1BFoDa8Oyc9j91L/7gHNTqLuIiMSkHTDKgNWx+TUhrZqZ\ndQLOdfdfErsbrrtvBn4GrCIKFO+5+5ywuL27bwj51gPtU2uBiIgAyZ7pnbZbgfjYhgGYWVdgPNHj\nYN8HHjKz0e7+YI5t1HKTlYmx6WHhJSIiGZWVlVRWVtaZL9WbD5rZScBEdz8jzE8A3N0nxfIsz0wC\nRwBbgEuBA4FT3f1bId/Xgc+6+zgzWwwMc/cNZtYRmOvuvXKUr5sPFrbEIm5fMbcN1L4Cl9YM2tcY\nNx9cAHzazDqHM5zKgZnxDO7eNbyOIxrHGOvuM4HXgJPM7ECL3p1T2P0s8ZnAN8L0hcCMlNshItLs\npdol5e67zGwc8ARRcLrH3Reb2WXRYr8re5XYui+Y2f3A34FdwPNAJv8kYJqZXQysBEal2Q4REdHz\nMApdog6LC1mauqQKWaLaV8jSmkH79DwMERHZZwoYIiKSiAKGiIgkooAhIiKJKGCIiEgiChgiIpKI\nAoaIiCSigCEiIokoYIiISCIKGCIikogChoiIJKKAISIiiShgiIhIIgoYIiKSiAKGiIgkooAhIiKJ\nKGCIiEgiChgiIpKIAoaIiCSigCEiIokoYIiISCIKGCIikogChoiIJKKAISIiiShgiIhIIgoYIiKS\niAKGiIgkooAhIiKJpB4wzOwMM1tiZkvN7Jpa8g02sx1mNiLMdzez581sYfj7vpl9Jyy7zszWhGUL\nzeyMtNshItLctUxz42bWApgMnAKsAxaY2Qx3X5Ij303A45k0d18KDIwtXwNMj612i7vfkmb9RURk\nt7SPMIYAr7v7SnffAVQA5+TIdznwEPB2nu18CXjD3dfE0qygNRURkVqlHTDKgNWx+TUhrZqZdQLO\ndfdfkj8InA9MzUobZ2aLzOzXZnZooSosIiK5NYVB71uB+NhGjaBhZqXAcOD3seQ7gK7uPgBYD6hr\nSkQkZamOYQBrgWNj80eHtLjPABVmZsARwJfNbIe7zwzLvwz83d3fyawQnwbuBmblr8LE2PSw8BIR\nkYzKykoqKyvrzGfunlolzKwEeI1o0PstYD5wgbsvzpN/CjDL3afH0qYCj7n7fbG0ju6+PkyPBwa7\n++gc23NIr317MtLcn3uUZobaV7DSirhtoPYVuLRm0D5332OIINUjDHffZWbjgCeIur/ucffFZnZZ\ntNjvyl4lPmNmrYkGvC/NynezmQ0AqoAVwGVp1F9ERHZL9QijsekIo+AlFnH7irltoPYVuLRm0L5c\nRxhNYdBbRET2AwoYIiKSiAKGiIgkooAhIiKJKGCIiEgiChgiIpKIAoaIiCSigCEiIokoYIiISCIK\nGCIikogChoiIJKKAISIiiShgiIhIIgoYIiKSiAKGiIgkooAhIiKJKGCIiEgiChgiIpKIAoaIiCSi\ngCEiIokoYIiISCIKGCIikogChoiIJKKAISIiiShgiIhIIgoYIiKSiAKGiIgkooAhIiKJpB4wzOwM\nM1tiZkvN7Jpa8g02sx1mNiLMdzez581sYfj7vpl9JyxrZ2ZPmNlrZva4mR2adjtERJq7VAOGmbUA\nJgOnA72BC8ysZ558NwGPZ9Lcfam7D3T3QcCJwBZgelg8AXjK3XsAfwKuLWzNKwu7uSansrErkLLK\nxq5AiiobuwIpq2zsCqSssrErUC9pH2EMAV5395XuvgOoAM7Jke9y4CHg7Tzb+RLwhruvCfPnAPeF\n6fuAcwtXZdjf39S6VTZ2BVJW2dgVSFFlY1cgZZWNXYGUVTZ2Beol7YBRBqyOza8JadXMrBNwrrv/\nErA82zkfmBqbb+/uGwDcfT3QvmA1FhGRnJrCoPetQHxso0bQMLNSYDjw+1q24SnUS0REYsw9ve9a\nMzsJmOjuZ4T5CYC7+6RYnuWZSeAIorGKS919Zlg+HBib2UZIWwwMc/cNZtYRmOvuvXKUr0AiIrIP\n3H2PHp+WKZe5APi0mXUG3gLKgQuyKtU1M21mU4BZmWARXEDN7iiAmcA3gEnAhcCMXIXnarCIiOyb\nVLuk3H0XMA54AngFqHD3xWZ2mZldmmuV+IyZtSYa8J6elW8ScKqZvQacQnSGlYiIpCjVLikRESke\nTWHQu9GZ2blmVmVm3cN8ZzN7KUwPNbNZDViXKjP7aWz+KjP7r71Y/zoz+24B6nGFmR2YZ9lcMxtU\n3zLqKL9e+2Efyqt+z9NUgPf3bDO7Op3a5S2zzMweDRffvm5mPzezOruzzexNMzssYRkF2f/h//Vz\nsfl6/T80RNv3JwoYkXLgD9QcX/E802nbBoxoAh+2K4HWjVh+Y+yHhnif69Uud5/l7jcXuE51mQ5M\nd/fuQHegDfDjBOvt7f4sxP4fBvxDAbaT0VBtr1O4wLlRNXoFGpuZHQx8Fvg3osDR2HYCdwF7/CoK\nv8LmmNkiM3vSzI7Os40BZvbXcOuUb8bW/56ZzQ/rXxfSWpvZH8LtV140s38xs8uBTsBcM5tTW2XN\n7JJQzjNmdpeZ3bbPLa+pXvsh/LK8P89++KmZvWRmL5jZqALVN6n6tutCM7s9TE8xszvM7G9mtszM\nhpnZ/5rZq2Z2b2ydD83sFjN7OWz38KSVNbOTgU/c/X6ITnEExgMXm9mBoT4Pm9nssJ8n5djG9WZ2\nRWz+xvAZy1ZqZr8J9Z+WOcI1s0FmVmlmC0I5HUL6d8zslbC/HrTo5Jp/Ba606JZCn8+qx+Dwni80\ns5vrOqJpqLaH931xnra/aWY3mdlzwEgz6xrKW2Bmf7bQK9Jg3L1Zv4DRwK/C9J+BgUBn4MWQNhSY\n2YD1+QA4BHiT6NfMVcB/hWUzga+F6YuAR3Ksfx3wPNAKOBxYBXQETgXuDHkMmAV8ARiRSQ/L2oS/\ny4F2eeo4FxgEHBXqeShQAswDbmvi+2EE8HjI0x5YCXSIv+dN/P29MLOPgSnAg2F6eNj2CWH+OaBf\nmK4CysP0fwK370V9Lwd+liP970CfUJ9loU0HACuAspDnTeCwsG//HvvsLcv+bIU8VcBJYf4eoqDa\nEvg/4PCQPgq4J0yvBUrDdNvY+/7drM/Bd8P0S8CQMP2Tut7vxm57bDvfi+V9Cjg+TA8B5qT9mY2/\nmv0RBlE31LQw/XuiANKo3P0jolueXJG16HPsPsX4AaIv/FxmuPt2d3+X6F5bQ4DTiM4sWwgsBHoA\n3Yj+iU41s5+Y2Rfc/cOwDSP/lfcZQ4BKd3/fozPiaru4cq+lsB8+G/JODdt/m+heDYMLWe+6FKBd\ncZnxtZeAt9z91TD/CtAlTFex+zP+G6DGL+99FP9szHH3j9x9G/Aq0RdgNXdfCWw0s/5En8OF7r45\nxzZXufszsXp+gehz2gd40syeB35AdPQL8ALwoJl9FdhVa2WjG5Qe4u7zQ9KDCduZc3Ox6TTbnvG7\n0IaDibrbfh/2xZ1EP3YaTNrXYTRpZtYOOBnoY9FFfiVEfY//06gVi/yC6It9Siwtu180Xz9pPN1i\n8z9x97uzM1s0gH0mcKOZPeXuN+5FPdO+1qWQ+6EqR57GulanPu2K2xb+VsWmM/P5/r/3pn/9VWBk\nPMHM2gLHEP1aPjGr3F15yv010VFTR+DeHMtz1cuJ3p+X3T1XkDsL+Ceio6sfmFmfWluy9+91Q7Y9\nW3xfbAl/WwCbPboha6No7kcY/wLc7+7HuXtXd+9MdAh4DI33RWIA4VfINOCS2LK/sntg/mvA03m2\ncY6ZtQp91UOJLqB8gqjv9WCI7uFlZkea2VFE/bQPAj8l6mqCqHujbR11XQD8k5kdatGZI+ftRTvr\nktZ+eBo438xamNmRwD8CmV+dDfGeF6JdtW47hxbs/uL7KvCXpBt09znAQWb2NQAzKwH+G5ji7lv3\nom6PAmcAnyF2V+osnc3ss2F6NFH7XwOOtOiuEZhZSzM7IeQ51t3/THT36rZEXUMfkuNz6+7vAx+Y\nWeZoss7xygZu+7E52p5dnw+BN82sOoiZWb+9qEe9NfeAcT7wSFbaw0S3S8/1a7QhxH9Z/Iyo/z2T\n9h3gIjNbRPSPn92lkfEiUVfLX4Eb3H29uz9JdBj+NzN7kaj76BCgLzA/HOL+F5A5urgbeMxyD3o7\ngLuvIzpjZD7RB/xN4P29bXAeae2HR0L6C0T9wd8PXVPZZaalEO3Kta3s+exfqEPCIO8w4Ia9rPM/\nA6PMbCmwBPiEqGuorjpVT3t0t+q5wDQPHfA5LAH+zcxeBT5FNLa4gyjYTQr75Xngc+EHym/M7AWi\nMYVfuPsHRF10/xwb9I6X9U3g16FbtjXJPqsN1fbXstueY5sQfS4uCQP9LxMdXTUYXbgn9WJmB7v7\nlvDr6xGiAcmct2pp4HpdB3zo7rc0dl0am5l96O5tGrkOLYi+2Ee6+xuNVIeD3X1LmL4G6Oju4xug\n3FrbHs7u+oO79027LvXV3I8wpP4mhqOTl4DlTSFYyB4a9VehmfUCXgeebKxgEZxl0enjLxENKu/N\nWN0+2Yu27xe/3HWEISIiiegIQ0REElHAEBGRRBQwREQkEQUMERFJRAFDREQSUcAQEZFE/j/8PtL1\nAqmUjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x146cb8b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw bar char\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 6\n",
    "accuracy_value = (0.486995019369, 0.486441615938, 0.491422246818, 0.484228002214, 0.485334809076, 0.484228002214)\n",
    "\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.5       # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, accuracy_value, width, color='b')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy of different combination')\n",
    "ax.set_xticks(ind + width/2)\n",
    "ax.set_xticklabels(('All', 'No best lg', 'No pol', 'No imp', 'Only bestlg', 'Only pre'))\n",
    "ax.set_ylim(ymin=.47, ymax=.5)\n",
    "def autolabel(rects):\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., 1.01*height,\n",
    "                '%.4f' % height,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only preprocess accuracy\n",
      "0.511990479024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.45      0.28      0.34      2581\n",
      "          0       0.50      0.61      0.55      7203\n",
      "          1       0.54      0.50      0.52      7021\n",
      "\n",
      "avg / total       0.51      0.51      0.51     16805\n",
      "\n",
      "Improved accuracy\n",
      "0.51329961321\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.46      0.28      0.35      2581\n",
      "          0       0.50      0.61      0.55      7203\n",
      "          1       0.54      0.50      0.52      7021\n",
      "\n",
      "avg / total       0.51      0.51      0.51     16805\n",
      "\n",
      "Add polarity accuracy\n",
      "0.514370722999\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.46      0.29      0.36      2581\n",
      "          0       0.50      0.61      0.55      7203\n",
      "          1       0.55      0.50      0.52      7021\n",
      "\n",
      "avg / total       0.51      0.51      0.51     16805\n",
      "\n",
      "Improved and polarity accuracy\n",
      "0.514549241297\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.46      0.30      0.36      2581\n",
      "          0       0.50      0.61      0.55      7203\n",
      "          1       0.55      0.49      0.52      7021\n",
      "\n",
      "avg / total       0.51      0.51      0.51     16805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "data_list = [train_data, imtrain_data, train_datapol, imtrain_datapol]\n",
    "dataname = ['Only preprocess', 'Improved', 'Add polarity', 'Improved and polarity']\n",
    "for i in range(4):\n",
    "    predictions = cross_validation.cross_val_predict(my_bestclf, data_list[i], trainlabel, cv=10)\n",
    "    print dataname[i] + \" accuracy\"\n",
    "    print accuracy_score(trainlabel,predictions)\n",
    "    print classification_report(trainlabel,predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "According to the bar char above and all the tune process, I can draw the following conclusion:\n",
    "\n",
    "1. My tuned best logistic regression model has very little positive effect on the performance of prediction. By observing the result from bar char, we can find that there is only small difference between my best logistic regression model and the default one. When comparing the accuracy of “Preprocess, improved, add polarity and best model” and “Preprocess, improved, add polarity and default model”, the accuracy is 0.486995 and 0.486442 on test data. My best model improved, but the improvement is not that significant.\n",
    "\n",
    "2. My improvement works. By removing some meaningless features, the performance improves most by comparing with other options. Still by observing the result on test data of different combination. The basic is all the combination has preprocess. The improvement is actually a deeper preprocess. It can be done after preprocess. By comparing the result using “preprocess, improved best model” and “ preprocess, best model”, the performance has a much more obvious improvement, adding from 0.4842 to 0.4914. \n",
    "\n",
    "3. Adding polarity feature to the feature dictionary have effect on the performance, but the effect can flutuate. When removing “add polarity” from all options, and adding “ add polarity” from “preprocess, and best model” , the performance on the test dataset both dropped. And by comparing result of  adding polarity and no polarity on the dev dataset, the performance of the same classifier raise from 0.50r to 0.505. From these two examples, we can find that adding polarity can have flutuate influence on the performance of the classifier. But the reason may come from using bad polarity lexicon. The accuracy of the using lexicon is only around 0.45. This really effect the result of polarity. By using a better polarity lexicon, adding polarity as a feature may have great chance to have positive influence on the performance of the classifier.\n",
    "\n",
    "From this assignment, I learned how to make a feature and how the adjust the classifier of a better result. Different arguments of the classifier may have significant effect on the performance of the classifier. Besides, by making my own logistic regression. I have a more clear understanding on logistics regression classifier. In the polarity process, I learned how the extract and generate a new feature to help build better feature dictionary. \n",
    "\n",
    "When talking about how to improve the performance of the system, I can draw three points:\n",
    "\n",
    "1. Do more proper and useful preprocess before building feature dictionary. We should remove as many useless and meaningless words as possible from the tokens, such as CONJ, PRONOUN, ADP and so on. What’s more, we need better solution to give back the origin words of abbreviation. Otherwise, words like “can’t”, “won’t” will be wrongly split by punctuation. \n",
    "\n",
    "2. Adding polarity as a feature, we need to build a better and more accurate lexicon to help judging the polarity of the instance. If the lexicon is not good enough, the performance of the classifier may drop.\n",
    "\n",
    "3. Try to find best combination of parameters of the classifier. The parameter combination can have significant influence on the performance of the system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
